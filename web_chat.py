import json
import streamlit as st
import torch
from transformers import AutoTokenizer
from models.model_llama import Transformer  # å¯¼å…¥æœ¬åœ°æ¨¡å‹
from models.LMConfig import LMConfig  # å¯¼å…¥é…ç½®ç±»
import os

st.set_page_config(page_title="Dylan LLM 26M")
st.title("Dylanè®­ç»ƒçš„å¤§æ¨¡å‹ 26M æ— ä¸Šä¸‹æ–‡")


model_id = "DylanLLM"

# -----------------------------------------------------------------------------
temperature = 0.7
top_k = 8
max_seq_len = 1 * 1024
# -----------------------------------------------------------------------------

@st.cache_resource
def load_model_tokenizer():
    # ä½¿ç”¨ LMConfig åˆå§‹åŒ–æ¨¡å‹é…ç½®
    lm_config = LMConfig()
    lm_config.max_seq_len = 1024  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰

    # åˆå§‹åŒ–æ¨¡å‹
    model = Transformer(lm_config).to('cuda' if torch.cuda.is_available() else 'cpu')

    # åŠ è½½æ¨¡å‹æƒé‡
    model_path = './out/full_sft_512.pth'
    state_dict = torch.load(model_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')
    model.load_state_dict(state_dict)
    model.eval()

    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained('./models/tokenizer_model', use_fast=False)

    # è¿”å›æ¨¡å‹å’Œåˆ†è¯å™¨
    return model, tokenizer, None

def clear_chat_messages():
    del st.session_state.messages


def init_chat_messages():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("Hello~æˆ‘æ˜¯Dylanåˆ©ç”¨å¼€æºèµ„æ–™è®­ç»ƒçš„å¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ˜„")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = "ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


def main():
    model, tokenizer, _ = load_model_tokenizer()
    messages = init_chat_messages()
    lm_config = LMConfig()
    lm_config.max_seq_len = 1024  # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰

    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
            messages.append({"role": "user", "content": prompt})

        with st.chat_message("assistant", avatar='ğŸ¤–'):
            placeholder = st.empty()

            chat_messages = []
            chat_messages.append({"role": "user", "content": prompt})

            new_prompt = tokenizer.apply_chat_template(
                chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )[-(lm_config.max_seq_len - 1):]

            x = tokenizer(new_prompt).data['input_ids']
            x = (torch.tensor(x, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')[None, ...])

            response = ''
            with torch.no_grad():
                res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=lm_config.max_seq_len, temperature=0.7, top_k=8, stream=True)

                try:
                    y = next(res_y)
                except StopIteration:
                    return

                history_idx = 0
                while y is not None:
                    answer = tokenizer.decode(y[0].tolist())
                    if answer and answer[-1] == 'ï¿½':
                        try:
                            y = next(res_y)
                        except:
                            break
                        continue

                    if not len(answer):
                        try:
                            y = next(res_y)
                        except:
                            break
                        continue

                    placeholder.markdown(answer)
                    response = answer
                    try:
                        y = next(res_y)
                    except:
                        break

        messages.append({"role": "assistant", "content": response})

    st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_messages)



if __name__ == "__main__":
    main()