{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39c1af50-6e1c-4397-91a7-0ba2cad5dd9b",
   "metadata": {},
   "source": [
    "- **从补全模型到对话模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f79022-647c-44fd-9103-a034f7ec1c58",
   "metadata": {},
   "source": [
    "&emsp;&emsp;全量指令微调（Full Instruction Tuning）在现代大规模预训练模型的应用中具有重要的意义。虽然预训练模型在大量的无监督数据上获得了强大的语言理解和生成能力，但这些模型通常缺乏处理具体任务的针对性表现。全量指令微调的核心作用在于将预训练模型进一步调整，使其能够根据明确的指令执行特定任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0869cf-69fd-4b4c-8087-e3255c02e429",
   "metadata": {},
   "source": [
    "- **全量指令微调的必要性**\n",
    "\n",
    "1. **增强模型的任务适应能力**：\n",
    "   - 预训练阶段的语言模型往往是通用的，并没有针对某个特定任务进行优化。虽然它们具备强大的生成和理解能力，但在实际应用中，我们常常需要模型执行某些具体的任务，例如对话生成、问答、文本摘要、翻译等。全量指令微调通过在明确的指令或任务定义下对模型进行进一步优化，提升了模型对这些任务的适应能力。\n",
    "\n",
    "2. **提升模型对多样化任务的执行能力**：\n",
    "   - 指令微调的一个关键特征是它能够帮助模型学会根据不同的输入指令生成相应的输出。通过微调，模型能够处理广泛的任务类型，并根据指令灵活应对不同的任务需求。这个过程让模型从一个纯语言生成器转变为一个能够执行多种任务的通用人工智能系统。\n",
    "\n",
    "3. **缩小预训练模型与应用场景的差距**：\n",
    "   - 预训练阶段的数据大多来自通用的文本数据，而模型实际应用中的任务和数据形式可能与预训练数据差异较大。通过全量指令微调，模型可以学习到如何在特定任务下调整其生成和决策方式，确保在实际应用中的表现更加精准和高效。这种微调过程弥补了预训练模型和实际应用场景之间的差距。\n",
    "\n",
    "4. **减少下游任务的数据需求**：\n",
    "   - 全量指令微调使得模型在接受指令后，能够直接执行多个任务，而不再需要为每个任务单独进行微调或标注大量数据。通过在多任务、多领域的指令数据上进行微调，模型能够以较少的数据应对大量的任务，提升了其广泛的适用性。\n",
    "\n",
    "5. **提高用户交互的可控性**：\n",
    "   - 通过指令微调，模型可以更好地理解和执行用户的明确需求，这使得模型的输出更加符合用户预期。用户可以通过简单明确的指令控制模型行为，而不再依赖复杂的上下文提示。这种可控性对于提供高质量的自动化服务和用户交互尤为重要。\n",
    "\n",
    "&emsp;&emsp;全量指令微调是使预训练模型更加实用和高效的关键步骤。它不仅提升了模型的任务执行能力，还使得模型更加灵活、可控和广泛适应多样化的应用场景。这使得指令微调成为在现代大规模语言模型开发和应用中不可或缺的一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948911a9-23fd-4bcc-a9fe-296e543417cf",
   "metadata": {},
   "source": [
    "- **预训练模型和指令微调模型不同的适用场景**\n",
    "\n",
    "&emsp;&emsp;不过需要注意的是，其实**预训练模型**和**指令微调模型**各自都有不同的适用场景，尽管指令微调模型在特定任务上表现更好，但预训练模型也有其独特的优势和适用领域。接下来，我们可以比较它们各自适用的场景，并解释为什么在某些情况下预训练模型仍然有它的价值。\n",
    "\n",
    "##### 预训练模型的适用场景\n",
    "\n",
    "1. **通用语言理解任务**：\n",
    "   - **场景**：预训练模型经过大量无监督文本数据的训练，具备广泛的语言理解能力，能够在没有明确任务定义的情况下生成文本或做出推理。\n",
    "   - **例子**：生成文章、写作辅助、文本补全等需要较强语言理解和生成能力的任务。\n",
    "   - **原因**：预训练模型没有被限制在特定任务上，因此它可以很好地处理广泛的语言生成需求，并且在一些开放领域的应用中表现出色。\n",
    "\n",
    "2. **低资源或无监督学习任务**：\n",
    "   - **场景**：在数据有限或者缺乏标注数据的场景下，预训练模型可以直接应用于一些无监督或少量数据的任务，如文本分类、情感分析等。\n",
    "   - **例子**：当你没有足够的任务数据进行微调时，预训练模型可以通过自监督学习生成文本或进行特征提取。\n",
    "   - **原因**：预训练模型的核心优势在于它从大量无标注数据中学到的广泛的语言模式，这让它即使在没有具体任务数据的情况下也能展现出良好的表现。\n",
    "\n",
    "3. **跨领域或未知任务的探索**：\n",
    "   - **场景**：当你需要在未知领域或者全新任务上探索模型的表现时，预训练模型可以作为一个强大的基础模型进行初步的探索和实验。\n",
    "   - **例子**：如果你需要构建一个多领域的生成任务，预训练模型可以提供灵活性，允许你在不同任务之间切换，而不需要进行专门的任务微调。\n",
    "   - **原因**：由于预训练模型没有被微调锁定在某一个任务上，它可以应用于不同领域、不同形式的输入，而不受任务特定的限制。\n",
    "\n",
    "4. **探索性的文本生成**：\n",
    "   - **场景**：当任务的目标是生成富有创造力、探索性或多样化的文本时，预训练模型由于其没有特定任务的约束，可能会生成更具多样性和创造力的文本。\n",
    "   - **例子**：文学写作、诗歌创作、广告文案等需要高自由度生成的任务。\n",
    "   - **原因**：预训练模型的多样化语言生成能力让它能够在这些没有明确限制的场景中表现得更加灵活。\n",
    "\n",
    "##### 指令微调模型的适用场景\n",
    "\n",
    "1. **明确任务驱动的场景**：\n",
    "   - **场景**：在指令微调模型中，模型经过了明确的任务定义和微调，能够很好地理解并执行用户给定的指令。\n",
    "   - **例子**：问答系统、文本摘要、机器翻译等有明确指令的任务。\n",
    "   - **原因**：指令微调模型能够根据特定任务中的指令生成高质量、符合任务需求的输出。它通过微调，在特定任务上表现更加精确。\n",
    "\n",
    "2. **多任务统一处理**：\n",
    "   - **场景**：在需要统一处理多个任务的情况下，指令微调模型非常适合。这些模型能够根据指令动态适应不同的任务需求，而无需为每个任务单独训练不同的模型。\n",
    "   - **例子**：集成多种能力的智能助手，能够根据不同的指令生成对话、翻译文本、生成摘要等多种任务的结果。\n",
    "   - **原因**：指令微调模型通过在多任务数据上的训练，具备根据不同指令处理不同任务的能力，这使得它可以灵活应对复杂的多任务场景。\n",
    "\n",
    "3. **高精度特定任务**：\n",
    "   - **场景**：当任务需要高精度的结果，例如法律文档分析、医疗问答等具有高专业性要求的任务时，指令微调模型可以通过微调后的知识提升在特定任务上的表现。\n",
    "   - **例子**：法律问题解答、医疗诊断建议生成等任务。\n",
    "   - **原因**：指令微调模型可以在这些任务特定的领域数据上进行进一步的优化，从而在输出结果的准确性和专业性上更有保障。\n",
    "\n",
    "4. **高效用户交互**：\n",
    "   - **场景**：在需要与用户进行高效交互的系统中，指令微调模型能够准确理解用户的意图，并根据具体指令生成用户期望的答案或结果。\n",
    "   - **例子**：智能客服系统、虚拟助手、对话生成等任务。\n",
    "   - **原因**：指令微调模型通过学习用户指令的意图，能够更好地理解并生成符合用户预期的结果，提升交互体验。\n",
    "\n",
    "##### 预训练模型 vs. 指令微调模型适用场景的比较\n",
    "\n",
    "| 特点                           | 预训练模型                                                                                      | 指令微调模型                                                                             |\n",
    "|------------------------------|-------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
    "| **任务灵活性**                | 适用于通用任务，能在没有特定任务数据的情况下进行探索性生成和任务处理                               | 针对明确任务进行优化，能根据特定指令执行高效、精确的任务                                |\n",
    "| **任务精确性**                | 生成的内容更加通用，适用于开放领域的任务                                                            | 生成内容更加准确，特别适合需要精确输出的应用场景                                          |\n",
    "| **多任务处理能力**            | 可处理广泛的任务，但没有经过微调，在特定任务上的表现可能不如微调模型                                   | 能根据指令灵活处理多个任务，在多任务环境中表现优秀                                       |\n",
    "| **资源要求**                  | 通常预训练模型在推理时资源需求较大，但在未微调的场景下灵活度较高                                       | 微调后模型在指定任务上表现出色，但可能在资源上有较高要求，尤其是多任务微调的情况下          |\n",
    "| **无监督学习应用**            | 在没有标注数据的场景下，预训练模型可以直接用于无监督任务                                              | 通常需要在标注数据上进行微调，因此在标注数据不足的情况下，微调模型不如预训练模型适用        |\n",
    "| **用户交互**                  | 可以根据通用的用户输入生成合理的文本，但缺乏对特定指令的理解                                           | 能更好地根据用户的指令执行任务，提升用户交互体验                                          |\n",
    "| **探索性和创造性**            | 更加适合开放性任务，能生成探索性、创造性较强的文本                                                    | 在任务明确时表现优异，但在过于开放的任务中，生成结果的多样性和创造力可能不如预训练模型       |\n",
    "\n",
    "##### 总结：\n",
    "- **预训练模型**：适用于开放领域任务、通用生成、低资源场景、跨领域探索和无监督学习任务。它在没有明确任务要求时非常灵活，可以直接应用于多个任务场景。\n",
    "- **指令微调模型**：适合有明确指令需求的任务、高精度的特定任务、多任务处理以及需要高效用户交互的应用。它在根据指令进行高效任务执行和准确性要求较高的任务中表现出色。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8504d2-e409-4bfd-88a0-23f15421bf53",
   "metadata": {},
   "source": [
    "- **全量指令微调的一般流程**\n",
    "\n",
    "1. **数据准备**：\n",
    "   - **任务定义与指令设计**：\n",
    "     - 在进行全量指令微调之前，必须明确微调模型需要执行的具体任务。这些任务可以是对话生成、问答系统、文本分类、机器翻译等。在数据准备阶段，需要为每个任务设计一组明确的指令，以指导模型的行为。例如，输入文本可以是 “给定一段话，请生成摘要”，而期望的输出是该段话的简洁概述。\n",
    "   - **多样化的指令数据集**：\n",
    "     - 指令微调的数据集通常包括多任务、多领域的数据，以增强模型的广泛适应能力。这些数据集可以来自多个来源，比如自然语言处理（NLP）任务数据集或用户交互记录。数据集的多样性越高，模型在面对不同任务时的表现通常越好。\n",
    "   - **指令格式化**：\n",
    "     - 在数据集中，输入的文本通常被格式化为“指令 + 输入”的形式，模型需要根据这组输入生成相应的输出。为了保持一致性，所有任务的数据都应遵循统一的格式标准，例如：\n",
    "       ```\n",
    "       指令: 给定以下问题，请生成合理的回答。\n",
    "       输入: 问题: 世界上最高的山是什么？\n",
    "       输出: 世界上最高的山是珠穆朗玛峰。\n",
    "       ```\n",
    "\n",
    "2. **模型配置**：\n",
    "   - **预训练模型加载**：\n",
    "     - 微调的基础是已经预训练好的大模型。这些模型通常具有通用的语言理解能力。加载预训练模型时，需确保模型具备良好的初始化权重，使其能够较快适应指令微调任务。\n",
    "   - **调整模型参数**：\n",
    "     - 在模型配置阶段，你可能需要根据指令微调任务的复杂性调整模型参数。比如，增加 `max_seq_len` 以处理较长的指令输入，或调整 `vocab_size` 以处理新的领域词汇。确保模型结构足够灵活，能够适应不同的任务需求。\n",
    "\n",
    "3. **指令微调**：\n",
    "   - **多任务微调**：\n",
    "     - 全量指令微调的一个特点是它通常在多任务数据上同时进行训练。这意味着模型将同时接触来自不同任务的数据，并根据不同的指令生成相应的输出。训练时，模型会学习如何根据不同类型的输入指令作出正确的响应。\n",
    "   - **损失函数设计**：\n",
    "     - 在微调阶段，损失函数的选择尤为重要。通常使用交叉熵损失函数来衡量模型输出与预期结果之间的差距。此外，如果任务之间的目标差异较大，可以对不同任务设置权重，从而引导模型重点关注某些特定的任务。\n",
    "   - **学习率和优化器**：\n",
    "     - 选择合适的学习率和优化器对模型的微调至关重要。一般来说，可以使用较小的学习率（例如 1e-5 或 5e-5）来避免对预训练模型权重进行过度修改。AdamW 是较为常见的优化器，因为它在大规模语言模型微调任务中表现稳定。\n",
    "\n",
    "4. **训练与监控**：\n",
    "   - **训练过程**：\n",
    "     - 在训练过程中，模型会根据指令和输入生成对应的输出，优化器会根据损失函数不断更新模型的权重。在这个过程中，确保模型训练稳定并且逐渐提高在各个任务上的表现。\n",
    "   - **使用监控工具**：\n",
    "     - 在训练过程中，建议使用监控工具（如 `TensorBoard` 或 `W&B`），实时监控损失曲线、训练进度等信息，以便及早发现潜在问题，例如模型过拟合或收敛不良。\n",
    "   - **验证集评估**：\n",
    "     - 在训练的过程中，定期使用验证集来评估模型在未见过的数据上的表现。验证集的设计应当与训练集保持一致，且最好覆盖多个任务场景，以确保模型的泛化能力。\n",
    "\n",
    "5. **模型评估与测试**：\n",
    "   - **评估模型性能**：\n",
    "     - 一旦模型完成微调，需要对其在多个任务上的性能进行评估。常用的评估指标包括准确率（Accuracy）、精确率（Precision）、召回率（Recall）、F1 分数等，具体选择取决于任务的性质。\n",
    "   - **测试实际应用场景**：\n",
    "     - 为了确保模型在真实场景中可以有效工作，可以通过实际的应用场景进行测试。比如，你可以通过模拟用户指令来测试模型在不同任务上的反应是否符合预期。\n",
    "   \n",
    "6. **部署与应用**：\n",
    "   - **模型导出与部署**：\n",
    "     - 完成微调后，模型可以导出为可部署的格式（例如 `TorchScript`、`ONNX` 等），并部署到生产环境中。\n",
    "   - **持续优化与反馈**：\n",
    "     - 在生产环境中，模型的表现会收到用户交互的影响，因此可以根据用户反馈进行进一步的微调和优化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406383bf-5abc-4bf0-a2f6-5888e2b7c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'instruction': '', 'input': '好的。现在请你将这个文本中的所有的逗号都替换成空格。', 'output': '好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？', 'history': [['给定一段文本和关键词列表，删除文本中包含所有给定关键词的子字符串。\\n文本：\"这是一个测试句子，目的是看看模型是否可以正确地从这个句子中删除关键词。\"\\\\n关键词列表：[‘测试’，‘模型’]', '删除包含所有给定关键词的子字符串后，文本变为：\"这是一个句子，目的是看看是否可以正确地从这个句子中删除关键词。\"']], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 59, 'output_len': 66, 'num_utter': 2, 'type': 31, 'type_keyword': ['字符串', '代码', '函数', '编写', '实现', '给定', '使用', '输入', '文本', '程序']}\n",
      "{'id': 2, 'instruction': '', 'input': '帮我回答一道历史题目。清朝时期的八旗共有多少旗人？', 'output': '清朝时期八旗旗人总数约为200万人左右，其中正黄旗、正蓝旗、正白旗、正红旗的人数较多，其他旗的人数较少。', 'history': [['写一个关于秋天的诗吧。', '红黄绿交织，田野金黄色。风吹枫叶落，人在秋天慢步行。'], ['给我推荐一些好听的摇滚歌曲。', \"摇滚歌曲推荐：AC/DC的Back in Black，Guns N' Roses的Sweet Child O' Mine，Rolling Stones的Satisfaction，Led Zeppelin的Stairway to Heaven，Nirvana的Smells Like Teen Spirit等。\"], ['计算一下2的100次方。', '2的100次方等于1.267650600228229401496703205376e+30，约等于1.27乘以10的30次方。']], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 15, 'output_len': 73, 'num_utter': 4, 'type': 11, 'type_keyword': ['生成', '诗歌', '主题', '爱情', '创作', '描述', '歌词', '表达', '歌曲', '有关']}\n",
      "{'id': 3, 'instruction': '', 'input': '嗯，谢谢你介绍的做法很详细，但我不喜欢吃鸡蛋，有没有其他菜做法能介绍一下？', 'output': '当然，你可以试试酸辣土豆丝这道菜。\\n材料：\\n土豆2个、红椒1个、青椒1个、大葱1根、醋、生抽、盐、鸡精、料酒\\n做法：\\n1.土豆去皮，切成丝；红椒和青椒切成细丝；大葱切段备用。\\n2.热锅凉油，油热后放入土豆丝，煸炒至变软。\\n3.倒入红椒、青椒和大葱段，继续煸炒至熟。\\n4.加入适量的盐、鸡精、料酒和生抽，翻炒均匀。\\n5.最后，加入适量的醋，翻炒均匀即可。\\n小贴士：\\n1. 土豆切丝时，可以放入淡盐水中泡一下，这样可以去除多余的淀粉。\\n2. 煮土豆丝时，不要煮得太久，以免烂糊。\\n3. 加入醋的时候，根据自己的口味多少来进行调节，一般来说，盐与醋的比例为1:1。\\n4. 如果喜欢辣味可以加入一些干辣椒丝。\\n希望你会喜欢这道酸辣土豆丝！', 'history': [['介绍一道好吃的家常菜的做法', '番茄炒蛋\\n材料：\\n鸡蛋3个、番茄1个、油、盐、糖、水淀粉\\n做法：\\n1.番茄洗净切成小块；鸡蛋打入碗中搅匀备用。\\n2.热锅凉油，油热后放入番茄翻炒。\\n3.番茄煮软后放盐，糖调味，再倒入打好的鸡蛋液，翻炒均匀。\\n4.鸡蛋凝固后关火。\\n5.加入半碗水，大火煮开。\\n6.用水淀粉勾芡，装盘即可。\\n小贴士：\\n1. 番茄不能切得太细，否则炒出来就不太好看了。\\n2. 在调味时，糖的分量也很重要，可以根据自己的口味来调节。\\n3. 水淀粉勾芡的比例是：水和淀粉的比例大概是5:1。\\n4. 煮开之后，记得要把水淀粉慢慢倒入，同时不断地搅拌，以免结块。\\n5. 最后装盘时可以点缀一些绿色蔬菜，既好看又好吃。']], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 25, 'output_len': 304, 'num_utter': 2, 'type': 47, 'type_keyword': ['制作', '食谱', '包括', '描述', '步骤', '烹饪', '提供', '材料', '食材', '菜谱']}\n",
      "{'id': 4, 'instruction': '', 'input': '请描述一下如何正确规划个人理财。', 'output': '正确规划个人理财需要以下几个步骤：\\n1.了解自己的财务状况。这包括收入、支出、资产和负债等信息。了解自己的财务状况可以帮助人们更好地制定财务计划。\\n2.设定财务目标。需要考虑短期目标和长期目标，例如以年为单位设定的支出计划、购房、购车等的长期目标。\\n3.制定预算计划。在了解自己的财务状况并设定财务目标后，需要制定一个预算计划。这可以帮助人们控制支出、节省开支并达到财务目标。\\n4.理性投资和储蓄。人们可以投资于股票、基金、房产或其他投资渠道以实现财务目标。但在投资前需了解相关知识并进行风险评估。同时还应储蓄一定金额，以应对突发事件或为达成某些目标做准备。\\n5.审时度势，合理调整。财务计划需要不断地审时度势，根据实际情况做出调整，以达到最终的财务目标。需要注意财务状况的变化、投资的收益和风险等因素。\\n通过以上五个步骤，人们可以做到合理规划个人理财，掌握自己的财务命运，更好地实现自己的财务目标。', 'history': [], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 16, 'output_len': 400, 'num_utter': 1, 'type': 26, 'type_keyword': ['提供', '建议', '问题', '提高', '方法', '帮助', '提出', '进行', '解决', '给出']}\n",
      "{'id': 5, 'instruction': '', 'input': '描述一下天堂和地狱的生态系统和环境。', 'output': '天堂和地狱被认为是灵性信仰中关于死后世界的两种不同概念。然而，它们的生态系统和环境都是具有类似特征的极端不同的地方。以下是我对天堂和地狱的生态系统和环境的描述。\\n天堂的生态系统和环境:\\n天堂被描绘为一个美丽、平静、和谐的地方，类似于一片无垢的花园。天堂的生态系统和环境的特征包括:\\n1. 充满和平和爱的氛围。这是一个没有恐惧、痛苦、疾病和死亡的地方。\\n2. 色彩缤纷，充满生机。这是一个绿树成荫、花团锦簇的地方，充满生机和活力。\\n3. 各种生物和动物和谐共存。天使、圣人和各种动物和谐相处，生态系统中没有互相侵害或抢夺资源。\\n4. 充满清新气息的空气。没有污染、烟雾或其他有害物质，空气中充满清新芬芳的气息。\\n5. 物质丰富的环境。天堂中生活着满足需求和愿望的人们，他们拥有一切所需的物质资源，而且没有匮乏、浪费或不公平。\\n地狱的生态系统和环境:\\n地狱被描绘为阴暗、恐怖、嘈杂和可怕的地方。地狱的生态系统和环境的特征包括:\\n1. 充满痛苦和折磨的氛围。这是一个充满恐惧、悔恨和痛苦的地方，全是罪恶的味道。\\n2. 火焰和烈火环绕。地狱中有燃烧的火焰和烈火，许多受罚者被投入火坑中痛苦折磨。\\n3. 恶魔和妖魔横行。地狱中有恶魔、妖怪等可怕的生物，它们在无休止的受苦中享受着自己的又一场比赛。\\n4. 污染和恶臭的气味。地狱中到处都是恶臭和污染，没有清新的气息。\\n5. 没有物质资源。地狱中生活着被惩罚的人们不可能拥有任何物质财富，地狱环境充满了无尽的贫困、饥饿和疾病。\\n综上所述，天堂和地狱是两个完全不同的地方，它们的生态系统和环境反映了它们的性质，体现了人类对不同阶段的死后生命的不同想象和信仰。', 'history': [], 'language': 'chinese', 'data_source': 'https://huggingface.co/datasets/BelleGroup/train_3.5M_CN', 'input_len': 18, 'output_len': 696, 'num_utter': 1, 'type': 23, 'type_keyword': ['解释', '描述', '概念', '提供', '包括', '说明', '给出', '应用', '介绍', '原理']}\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "# 查看JSONL文件的前几行\n",
    "file_path = '../data/raw/sft_data_zh.jsonl'\n",
    "\n",
    "with jsonlines.open(file_path) as reader:\n",
    "    for i, obj in enumerate(reader):\n",
    "        print(obj)  # 打印每一行\n",
    "        if i >= 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813ed3b-e882-437e-bd7d-536276967931",
   "metadata": {},
   "source": [
    "而其中我们只需要提取对话的信息即可：将符合条件的部分保存到一个 CSV 文件中。整个过程涉及到数据的筛选、清洗、处理和写入。具体步骤如下：\n",
    "\n",
    "1. **选择输入文件**：\n",
    "   - 根据参数 `contain_history`，确定要处理的数据集文件名为 `'sft_data.csv'` 或 `'sft_data_single.csv'`。\n",
    "\n",
    "2. **文本筛选函数**：\n",
    "   - `chinese_ratio(text)`：计算一个文本中中文字符的比例，用来筛选是否大部分内容是中文。\n",
    "\n",
    "3. **处理数据并写入 CSV**：\n",
    "   - `process_and_write_data(data)`：对数据进行筛选，检查每一条对话是否符合以下条件：\n",
    "     - 如果 `contain_history` 为真，则数据必须有对话历史。\n",
    "     - 对话的问句 (`q`) 和答句 (`a`) 都必须存在，并且它们的长度要在指定范围内（问句长度在 10 到 256 之间，答句长度在 5 到 256 之间）。\n",
    "     - 问句和答句的中文字符比例必须大于 90%。\n",
    "   - 如果数据符合条件，则将问句、答句和对话历史（如果有的话）添加到列表中，并写入 CSV 文件。\n",
    "\n",
    "4. **逐行读取 JSONL 数据集**：\n",
    "   - `sft_datasets` 包含了一个要处理的 JSONL 数据集文件路径。\n",
    "   - 使用 `jsonlines` 库逐行读取数据，并将问答对保存在 `data` 列表中。每当 `data` 列表中累计了 1000 条记录（`chunk_size`），就将数据进行处理并写入到 CSV 文件中。\n",
    "   - 如果读取的行有格式问题，代码会跳过并继续处理下一行。\n",
    "\n",
    "5. **进度条**：\n",
    "   - 使用 `tqdm` 库为整个处理过程添加进度条，显示处理数据的进度，并在每处理 `chunk_size` 条数据后更新进度。\n",
    "\n",
    "6. **输出结果**：\n",
    "   - 生成的 CSV 文件包括三个列：`history`（对话历史）、`q`（问句）、`a`（答句）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae23a55-2a4a-4b67-9b44-90842645258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f3e3a4-875d-4c77-9e69-6b3151dadbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "956c462c-c3df-4168-a66f-70867abe098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer词表大小 6400\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../models/tokenizer_model',use_fast=False)\n",
    "print('tokenizer词表大小', len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21eb6cde-8958-4126-af31-32441871b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # 导入 tqdm\n",
    "\n",
    "def sft_process(contain_history=False):\n",
    "    file_name = 'sft_data.csv' if contain_history else 'sft_data_single.csv'\n",
    "\n",
    "    def chinese_ratio(text):\n",
    "        \"\"\"计算文本中中文字符的比例。\"\"\"\n",
    "        chinese_chars = re.findall(r'[\\u4e00-\\u9fff]', text)  # 匹配中文字符\n",
    "        return len(chinese_chars) / len(text) if text else 0\n",
    "\n",
    "    def process_and_write_data(data):\n",
    "        \"\"\"处理数据并将其写入 CSV 文件。\"\"\"\n",
    "        q_lst, a_lst, history_lst = [], [], []\n",
    "        for per in data:\n",
    "            history, q, a = per['history'], per['q'], per['a']\n",
    "\n",
    "            # 数据筛选条件\n",
    "            if (contain_history and not history) or not q or not a:\n",
    "                continue\n",
    "            if len(q) < 10 or len(a) < 5:\n",
    "                continue\n",
    "            if len(q) > 256 or len(a) > 256:\n",
    "                continue\n",
    "            if not (chinese_ratio(q) > 0.9 and chinese_ratio(a) > 0.9):\n",
    "                continue\n",
    "\n",
    "            # 将有效数据添加到列表\n",
    "            q_lst.append(q)\n",
    "            a_lst.append(a)\n",
    "            if contain_history:\n",
    "                history_lst.append(history)\n",
    "            else:\n",
    "                history_lst.append([])\n",
    "\n",
    "        # 创建 DataFrame 并写入 CSV 文件\n",
    "        df = pd.DataFrame({'history': history_lst, 'q': q_lst, 'a': a_lst})\n",
    "        df.to_csv(f'./dataset/{file_name}', mode='a', header=False, index=False, \n",
    "                   lineterminator='\\r\\n', escapechar='\\\\', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    chunk_size = 1000  # 每次处理的记录数\n",
    "    data = []\n",
    "\n",
    "    # 创建 CSV 文件并写入表头\n",
    "    with open(f'./dataset/{file_name}', 'w', encoding='utf-8') as f:\n",
    "        f.write('history,q,a\\n')\n",
    "\n",
    "    sft_datasets = ['./dataset/sft_data_zh.jsonl']\n",
    "    \n",
    "    # 开始处理数据集\n",
    "    for path in sft_datasets:\n",
    "        with jsonlines.open(path) as reader:\n",
    "            total_lines = sum(1 for _ in open(path))  # 获取总行数\n",
    "            \n",
    "            # 使用 tqdm 创建进度条\n",
    "            with tqdm(total=total_lines, desc=\"Processing lines\") as pbar:\n",
    "                for idx, obj in enumerate(reader):\n",
    "                    try:\n",
    "                        data.append({\n",
    "                            'history': obj.get('history', ''),\n",
    "                            'q': obj.get('input', '') + obj.get('q', ''),\n",
    "                            'a': obj.get('output', '') + obj.get('a', '')\n",
    "                        })\n",
    "\n",
    "                        # 每达到 chunk_size，就处理并写入数据\n",
    "                        if len(data) >= chunk_size:\n",
    "                            process_and_write_data(data)\n",
    "                            data = []\n",
    "                            pbar.update(chunk_size)  # 更新进度条\n",
    "\n",
    "                    except jsonlines.InvalidLineError as e:\n",
    "                        print(f\"Skipping invalid JSON line {idx + 1}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # 处理剩余的数据\n",
    "                if data:\n",
    "                    process_and_write_data(data)\n",
    "                    pbar.update(len(data))  # 更新进度条\n",
    "\n",
    "    print(\"数据处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5e6a1-a598-4fad-95e3-bff649e917fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_process(contain_history=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e9914e-bc7a-4b47-9b21-9667127d94f3",
   "metadata": {},
   "source": [
    "##### 查看处理后的数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5200ec7f-9033-467b-b455-3e48ca9d45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  history                                      q  \\\n",
      "0      []             好的。现在请你将这个文本中的所有的逗号都替换成空格。   \n",
      "1      []              帮我回答一道历史题目。清朝时期的八旗共有多少旗人？   \n",
      "2      []                     请给我介绍一下人工智能的技术和发展。   \n",
      "3      []          很好，现在请你计算一下这篇文章中出现了多少个“获得”字样。   \n",
      "4      []  A公司去年亏损了500万美元，今年净利润增长了50%，今年的净利润是多少？   \n",
      "\n",
      "                                                   a  \n",
      "0  好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是...  \n",
      "1  清朝时期八旗旗人总数约为200万人左右，其中正黄旗、正蓝旗、正白旗、正红旗的人数较多，其他旗...  \n",
      "2  人工智能的技术发展已经越来越快，包括深度学习、机器学习、自然语言处理、计算机视觉等。随着算力...  \n",
      "3                         经过计数，这篇文章中总共出现了两次“获得”这个词汇。  \n",
      "4                                       今年净利润为750万美元  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载 CSV 文件\n",
    "file_path = '../data/processed/sft_data_single_large.csv'  # 确保路径正确\n",
    "\n",
    "# 使用 pandas 读取前 5 行\n",
    "df = pd.read_csv(file_path, nrows=5)\n",
    "\n",
    "# 显示前 5 行数据\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac18eed7-b9c3-451b-ac22-78bc7845aefb",
   "metadata": {},
   "source": [
    "#### 2.指令微调代码编写与代码解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30bd89e-9252-4de9-83c2-d7f5ff69fd69",
   "metadata": {},
   "source": [
    "- 指令微调代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a24ab-157a-48aa-8ee5-cc70e31c9371",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在准备好数据集之后，接下来即可开始指令微调。首先我们需要编写一个指令微调的脚本文件`full_sft.py`其中代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7648d-ba4a-4434-a030-2b5f8193c41b",
   "metadata": {},
   "source": [
    "- 指令微调代码解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153ff39-65b5-46a6-b857-afc8b5634238",
   "metadata": {},
   "source": [
    "接下来，我们分部分解释代码的主要功能和逻辑：\n",
    "\n",
    "#### 1. **导入依赖库**\n",
    "```python\n",
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from model.model import Transformer\n",
    "from model.LMConfig import LMConfig\n",
    "from model.dataset import SFTDataset\n",
    "```\n",
    "- 这部分导入了常见的库，如 `os`、`time`、`pandas`，以及用于深度学习训练的 PyTorch 库。\n",
    "- 主要导入了用于加载分词器和模型的 `transformers` 库（Hugging Face 的工具），以及自定义的 `Transformer` 模型、配置文件 `LMConfig` 和数据集处理模块 `SFTDataset`。\n",
    "\n",
    "#### 2. **日志记录函数 `Logger`**\n",
    "```python\n",
    "def Logger(content):\n",
    "    if not ddp or dist.get_rank() == 0:\n",
    "        print(content)\n",
    "```\n",
    "- **功能**：这个函数用于在分布式训练中，只在主进程上输出日志内容（因为在分布式环境中，可能会有多个进程执行同样的操作）。\n",
    "- **`dist.get_rank()`**：返回当前进程的 rank，只有 rank 为 0 的进程（主节点）才会输出日志。\n",
    "\n",
    "#### 3. **学习率调度函数 `get_lr`**\n",
    "```python\n",
    "def get_lr(it, all):\n",
    "    warmup_iters = args.warmup_iters\n",
    "    lr_decay_iters = all\n",
    "    min_lr = args.learning_rate / 10\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "```\n",
    "- **功能**：实现**余弦退火学习率调度**（Cosine Annealing Learning Rate）。在训练的早期，学习率先逐步升高（**warmup**），然后在训练后期逐步减小。\n",
    "- 这种调度策略有助于模型在训练中快速收敛，同时避免后期训练时步长过大导致不稳定。\n",
    "\n",
    "#### 4. **模型训练函数 `train_epoch`**\n",
    "```python\n",
    "def train_epoch(epoch, wandb):\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        with ctx:\n",
    "            logits = model(X, Y).logits\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), Y.view(-1), ignore_index=0, reduction='none')\n",
    "            loss_mask = loss_mask.view(-1)\n",
    "            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n",
    "                    epoch,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item(),\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "\n",
    "            if (wandb is not None) and (not ddp or dist.get_rank() == 0):\n",
    "                wandb.log({\"loss\": loss,\n",
    "                           \"lr\": optimizer.param_groups[-1]['lr'],\n",
    "                           \"epoch_Time\": spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60})\n",
    "\n",
    "        if (step + 1) % args.save_interval == 0 and (not ddp or dist.get_rank() == 0):\n",
    "            model.eval()\n",
    "            moe_path = '_moe' if lm_config.use_moe else ''\n",
    "            ckp = f'{args.save_dir}/full_sft_{lm_config.dim}{moe_path}.pth'\n",
    "\n",
    "            if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
    "                state_dict = model.module.state_dict()\n",
    "            else:\n",
    "                state_dict = model.state_dict()\n",
    "\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "```\n",
    "- **核心功能**：执行模型的单轮训练，计算损失并更新模型参数。\n",
    "- **重要步骤**：\n",
    "  - **学习率更新**：通过 `get_lr` 动态调整学习率。\n",
    "  - **损失计算**：使用 `cross_entropy` 损失函数，忽略 `ignore_index=0`（常用于忽略填充的 token）。\n",
    "  - **梯度累积**：通过 `args.accumulation_steps` 控制梯度累积，适用于较大模型的训练。\n",
    "  - **梯度裁剪**：使用 `clip_grad_norm_` 限制梯度的最大范数，防止梯度爆炸。\n",
    "  - **保存检查点**：定期保存模型权重到 `.pth` 文件中，支持恢复训练。\n",
    "\n",
    "#### 5. **模型初始化 `init_model`**\n",
    "```python\n",
    "def init_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./model/mateconv_tokenizer')\n",
    "    model_from = 1  # 1从权重，2用transformers\n",
    "\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    if model_from == 1:\n",
    "        model = Transformer(lm_config)\n",
    "        moe_path = '_moe' if lm_config.use_moe else ''\n",
    "        ckp = f'./out/pretrain_{lm_config.dim}{moe_path}.pth'\n",
    "        state_dict = torch.load(ckp, map_location=args.device)\n",
    "        unwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(state_dict.items()):\n",
    "            if k.startswith(unwanted_prefix):\n",
    "                state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained('./MateConv', trust_remote_code=True)\n",
    "\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    model = model.to(args.device)\n",
    "\n",
    "    return model, tokenizer\n",
    "```\n",
    "- **功能**：加载模型和分词器，支持从不同源加载模型（如自定义模型权重或 Hugging Face 的 `AutoModel`）。\n",
    "- **模型权重加载**：从 `./out` 目录中加载预训练权重（`.pth` 文件），并加载到 `Transformer` 模型中。\n",
    "- **参数统计**：计算模型的总参数量并输出。\n",
    "\n",
    "#### 6. **分布式训练初始化 `init_distributed_mode`**\n",
    "```python\n",
    "def init_distributed_mode():\n",
    "    if not ddp: return\n",
    "    global ddp_local_rank, DEVICE\n",
    "\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    ddp_rank = int(os.environ[\"RANK\"])\n",
    "    ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    DEVICE = f\"cuda:{ddp_local_rank}\"\n",
    "    torch.cuda.set_device(DEVICE)\n",
    "```\n",
    "- **功能**：初始化分布式训练环境，使用 NCCL 后端进行 GPU 通信。适用于多 GPU 或多节点训练。\n",
    "\n",
    "#### 7. **主流程**\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"MateConv Full SFT\")\n",
    "    ...\n",
    "    args = parser.parse_args()\n",
    "    ...\n",
    "    model, tokenizer = init_model()\n",
    "    ...\n",
    "    train_ds = SFTDataset(df, tokenizer, max_length=max_seq_len)\n",
    "    train_loader = DataLoader(...)\n",
    "    ...\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    ...\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch, wandb)\n",
    "```\n",
    "- **命令行参数解析**：使用 `argparse` 解析训练相关参数（如学习率、\n",
    "\n",
    "批次大小等）。\n",
    "- **数据加载**：使用 `SFTDataset` 处理数据集，`DataLoader` 负责将数据批量化用于训练。\n",
    "- **自动混合精度训练**：通过 `torch.cuda.amp.GradScaler` 实现半精度训练（如 `float16`），以减少显存占用。\n",
    "- **训练循环**：遍历训练的 epoch，调用 `train_epoch` 进行模型更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff70912e-631b-458d-a037-a26daa201c84",
   "metadata": {},
   "source": [
    "### 二、模型对话效果测试与高层模型调用API封装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951afc1-feff-423f-8c28-b2de63713e51",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在完成了模型指令微调之后，接下来即可测试模型对话效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10acd14-0874-47c8-8313-a440ba9dc69a",
   "metadata": {},
   "source": [
    "#### 1.使用transformer库进行模型推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a836dcf-e301-4b05-b627-9e1c7a7fc77e",
   "metadata": {},
   "source": [
    "- 对话测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464c3633-7463-4b07-ae6e-3a8ad4270b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5873bb97-dcff-40a8-b4ea-28e4fa5ed306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from models.model_llama import Transformer\n",
    "from models.LMConfig import LMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1971f26-064c-45a8-bf72-aeb3122232ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 设置设备和随机种子\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "setup_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef58b712-572f-4013-a9ad-6e6052608e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d2e6f-7c9e-4df9-b885-0958d3501d02",
   "metadata": {},
   "source": [
    "- **目的**：设置随机数种子是为了确保模型运行时的**确定性和复现性**。\n",
    "  - `random.seed`、`np.random.seed` 和 `torch.manual_seed` 用于固定 Python、NumPy 和 PyTorch 的随机数生成器。\n",
    "  - **设备选择**：优先选择 GPU 进行推理。如果 GPU 不可用，则使用 CPU。\n",
    "  - `cudnn.deterministic = True` 确保在使用 CUDA 加速时，结果可以复现，但会略微降低性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "022a4c15-efb5-40f9-8f72-bab34789e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 初始化模型和分词器\n",
    "lm_config = LMConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d401467c-1abd-42b5-adb9-bd5654420608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LMConfig {\n",
       "  \"aux_loss_alpha\": 0.01,\n",
       "  \"dim\": 512,\n",
       "  \"dropout\": 0.0,\n",
       "  \"flash_attn\": true,\n",
       "  \"hidden_dim\": null,\n",
       "  \"max_seq_len\": 512,\n",
       "  \"model_type\": \"DylanBaseModel(Llama)\",\n",
       "  \"multiple_of\": 64,\n",
       "  \"n_heads\": 16,\n",
       "  \"n_kv_heads\": 8,\n",
       "  \"n_layers\": 8,\n",
       "  \"n_routed_experts\": 4,\n",
       "  \"n_shared_experts\": true,\n",
       "  \"norm_eps\": 1e-05,\n",
       "  \"norm_topk_prob\": true,\n",
       "  \"num_experts_per_tok\": 2,\n",
       "  \"scoring_func\": \"softmax\",\n",
       "  \"seq_aux\": true,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"use_moe\": false,\n",
       "  \"vocab_size\": 6400\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a5e59-e29e-46dd-b3fd-56d2c539c017",
   "metadata": {},
   "source": [
    "这里确保\"dim\": 768或者512，才可以使用对应参数的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "993ded31-72dc-4f0b-8a9d-ac72aed01297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(6400, 512)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (wk): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wv): Linear(in_features=512, out_features=256, bias=False)\n",
       "        (wo): Linear(in_features=512, out_features=512, bias=False)\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (w2): Linear(in_features=1408, out_features=512, bias=False)\n",
       "        (w3): Linear(in_features=512, out_features=1408, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=512, out_features=6400, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 1024  # 可以根据需要调整\n",
    "lm_config.max_seq_len = max_seq_len\n",
    "\n",
    "model = Transformer(lm_config).to(device)\n",
    "model_path = '../out/full_sft_512.pth'  # 替换为你的模型路径\n",
    "state_dict = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f73d11c-3f0f-464c-bb32-d8f14f9d4a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer词表大小 6400\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../models/tokenizer_model', use_fast=False)\n",
    "print('tokenizer词表大小', len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aeaf69-8e42-4342-a65a-f59cab9aae10",
   "metadata": {},
   "source": [
    "- **模型加载**：\n",
    "  - `LMConfig`：初始化模型配置，例如设置 `max_seq_len` 表示模型的最大输入序列长度。\n",
    "  - **加载模型权重**：从指定的 `model_path` 路径加载已经微调好的模型权重 (`full_sft_512.pth`)。\n",
    "  - **模型评估模式**：`model.eval()` 切换模型为评估模式，禁用 dropout 和 batch normalization 的训练行为。\n",
    "- **分词器加载**：\n",
    "  - 分词器用于将输入的自然语言文本转换为模型所需的 token 序列，并可以将模型输出的 token 序列转回人类可读的文本。这里从 `mateconv_tokenizer` 路径加载分词器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90a1b8b2-e30e-42b8-ad1d-e97a61394d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 对话函数：生成完整回复\n",
    "def generate_reply(prompt, temperature=0.5, top_k=16, stream=True):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # 使用自定义的 prompt 模板 (根据你的应用逻辑)\n",
    "    new_prompt = tokenizer.apply_chat_template(\n",
    "        conversation = messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )[-(max_seq_len - 1):]\n",
    "\n",
    "    input_ids = tokenizer(new_prompt).data['input_ids']\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    generated_text = \"\"\n",
    "    with torch.no_grad():\n",
    "        # 生成器返回的生成结果\n",
    "        res_y = model.generate(input_ids, \n",
    "                               tokenizer.eos_token_id, \n",
    "                               max_new_tokens=max_seq_len, \n",
    "                               temperature=temperature, \n",
    "                               top_k=top_k, \n",
    "                               stream=stream)\n",
    "\n",
    "        # 从生成器逐步获取生成结果\n",
    "        try:\n",
    "            y = next(res_y)\n",
    "        except StopIteration:\n",
    "            print(\"No answer\")\n",
    "            return \"\"\n",
    "\n",
    "        history_idx = 0\n",
    "        while y is not None:\n",
    "            answer = tokenizer.decode(y[0].tolist())\n",
    "            if answer and answer[-1] == '�':\n",
    "                try:\n",
    "                    y = next(res_y)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            if len(answer):\n",
    "                generated_text += answer[history_idx:]\n",
    "            \n",
    "            try:\n",
    "                y = next(res_y)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            history_idx = len(answer)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749941d-a06b-4eb6-9290-4c95cfe5645f",
   "metadata": {},
   "source": [
    "- **功能**：这个函数用于生成对话回复，流程如下：\n",
    "  1. **自定义 `prompt`**：将用户输入（`prompt`）转换为自定义格式，这里使用了 `apply_chat_template` 生成对话的模板，将 `messages` 处理为可供模型生成的输入。\n",
    "  2. **生成 `input_ids`**：将 `new_prompt` 转换为 token 序列 (`input_ids`) 供模型使用，并且调整其形状以适应批处理（`unsqueeze(0)` 增加批次维度）。\n",
    "  3. **模型生成**：通过 `model.generate` 函数，使用贪心搜索或随机采样（受 `temperature` 和 `top_k` 控制）生成回复。\n",
    "     - `temperature` 控制生成时的随机性，值越小生成结果越确定，越大则越随机。\n",
    "     - `top_k` 限制采样的选择范围，取概率最高的 `k` 个 token。\n",
    "  4. **逐步解码**：模型生成结果会被逐步解码为可读文本，直到完成生成或遇到 `eos_token`。\n",
    "  5. **返回结果**：最终返回生成的完整回复文本。\n",
    "\n",
    "##### 参数说明：\n",
    "- **`prompt`**：用户输入的对话文本，模型将根据该输入生成回复。\n",
    "- **`temperature`**：采样时的随机性控制参数，较高的值会增加生成内容的多样性，较低的值会生成更确定的输出。\n",
    "- **`top_k`**：采样时只从概率最高的前 `k` 个 token 中选择，限制生成的词汇选择范围，避免生成低概率的单词。\n",
    "- **`stream`**：决定是否以流式方式逐步获取生成结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9797af38-90c6-4457-8e95-947859d19670",
   "metadata": {},
   "source": [
    "这里需要注意的是，**大模型判断回复停止的主要依据是 `eos_token`（End of Sequence token）**，即序列结束标记。`eos_token` 是用于告诉模型生成任务已经完成的特殊标记。模型在生成文本时，一旦生成了 `eos_token`，它就知道应该停止输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf41c10-80fa-467a-bb45-3f00432413be",
   "metadata": {},
   "source": [
    "大模型如何判断下一个单词（token）是 `eos_token`（序列结束标记），实际上是通过**模型的生成机制**，结合模型在**预训练和指令微调阶段学到的语言模式**来决定的。模型通过**概率分布**来选择下一个 token，而这个概率分布是在**预训练**和**指令微调**中学到的。\n",
    "\n",
    "#### 1. **生成机制的工作原理**：\n",
    "模型生成文本的过程通常基于自回归机制，也就是模型在生成每个 token 时，都会基于之前生成的所有 token 来预测下一个 token。它并不会明确知道下一个 token 是什么，而是通过计算每个可能 token 的概率分布，从中选择最可能的 token（包括 `eos_token`）。\n",
    "\n",
    "##### 步骤概述：\n",
    "1. **输入上下文**：给定一个输入（如用户问题或对话），模型会根据上下文生成一个概率分布，这个分布表示每个可能的 token 出现在该位置的概率。\n",
    "2. **选择下一个 token**：模型通过贪心搜索、随机采样（基于 `temperature`）或其他策略，选择下一个 token。`eos_token` 作为候选之一，它的概率也由模型预测出来。\n",
    "3. **生成停止**：当模型预测出 `eos_token` 并选择它作为下一个 token 时，生成过程停止。\n",
    "\n",
    "#### 2. **影响 `eos_token` 判断的因素**：\n",
    "\n",
    "##### (1) **预训练的影响**：\n",
    "预训练的过程中，模型会接触到大量的自然语言文本。通过自回归语言模型的方式，模型学会了**如何生成自然语言**，包括如何适时结束一段句子或文本。因此，预训练赋予了模型关于自然语言的**结构、语法、句法模式**的基础知识。\n",
    "\n",
    "- **预训练阶段的 `eos_token`**：在预训练的文本数据中，通常会在每个训练样本的末尾加上 `eos_token`，因此模型在预训练中学会了何时结束一段文本。在生成过程中，模型会根据上下文推测是否应该结束文本。\n",
    "\n",
    "##### (2) **指令微调的影响**：\n",
    "指令微调专门针对任务的要求进行了微调，模型不仅学习了如何生成回答，还学习了如何在不同的任务和上下文中生成适合的结束标记。因此，指令微调对 `eos_token` 的判断进一步进行了优化，使得模型在实际任务中能够更好地判断何时结束输出。\n",
    "\n",
    "- **指令微调阶段的 `eos_token`**：指令微调阶段的训练数据中，通常会标注任务的起始和结束，并提供明确的输入和输出。这使得模型能更好地理解在特定任务（如对话、问答等）中，何时应该生成 `eos_token` 来结束对话或回答。\n",
    "\n",
    "#### 3. **模型如何具体判断 `eos_token`**：\n",
    "\n",
    "在每个生成步骤中，模型会基于已经生成的 token 来预测下一个 token。预测的过程通常如下：\n",
    "\n",
    "1. **概率分布计算**：模型通过计算输入 token 序列的上下文信息，输出一个表示所有可能 token 的概率分布（Softmax）。这个概率分布表示每个 token 出现的可能性，包括 `eos_token`。\n",
    "   \n",
    "   例如，假设模型在某个位置预测下一 token 时，给出了以下概率分布：\n",
    "   - `the`: 0.4\n",
    "   - `a`: 0.3\n",
    "   - `eos_token`: 0.2\n",
    "   - 其他 token：0.1\n",
    "\n",
    "2. **选择 token**：根据生成策略（如贪心搜索、随机采样等），模型会选择概率最大的 token，或者根据设定的 `temperature` 或 `top_k` 进行采样。如果 `eos_token` 的概率最高或者通过采样选中，模型会生成 `eos_token`，表示生成过程结束。\n",
    "\n",
    "   - **贪心搜索**：选择当前概率最大的 token。\n",
    "   - **随机采样**：根据概率进行采样，`temperature` 控制采样的随机性。\n",
    "   - **top-k 采样**：只从概率最高的前 `k` 个 token 中选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c4a86-f108-4f6e-946a-225ebeda1aa2",
   "metadata": {},
   "source": [
    "## 测试问答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abb7f761-cce4-4aad-83a9-40a35e89ee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'长江是中国最长、最长的人口之一，也是世界第二大的人口。它是中国最重要的河流之一，也是中国最重要的经济和文化中心。长江流域是世界上最古老、最重要的经济和文化中心，也是中国最重要的工业和农业生产中心。长江流域地势险峻，气候宜人，是中国最重要的自然保护区之一。长江上游是中国最具代表性的文化和历史遗产之一，也是中国最著名的旅游景点之一。'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse = generate_reply(\"长江、\")\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22190f50-0b6c-46ea-8b01-283e788d5bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好，我很高兴能够帮助你。祝你有一个美好的一天！'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_reply(\"你好，好久不见！\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "60934f7b-5197-4f9d-b748-e0011024cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 机器学习是一种人工智能的分支，它使用算法和统计模型来让计算机系统从数据中学习，从而不断提高性能。机器学习可以分为有监督学习、无监督学习和强化学习三种类型。有监督学习是指使用有标记的数据来训练模型，无监督学习则是使用无标记的数据来训练模型，强化学习则是通过奖励或惩罚来训练模型。机器学习在图像识别、自然语言处理、语音识别等领域都有广泛的应用。'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_reply(\"请问什么是机器学习？\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78eabbae-3718-4f4f-933f-c2f1107039e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'机器学习是人工智能的一个分支，它使用算法和统计模型来让计算机系统从数据中进行学习和自我完善，而深度学习是机器学习的子集，它使用多层神经网络来学习数据的特征表示，从而实现更高的准确性和效率。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = generate_reply(\"请问机器学习和深度学习的区别是什么？\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7a39c-b3bc-4ef0-9e57-6fac222b3c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
