{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e10140-8cbd-4080-82de-9375db408d00",
   "metadata": {},
   "source": [
    "#### 数据集可以选择市面上容易得到的数据集，例如维基百科中文文档，中文悟道开源的数据集，这个可以根据个人选择。\n",
    "\n",
    "&emsp;&emsp;在大模型训练的过程中，预训练阶段是至关重要的一步。**预训练**（Pre-training）是指在大规模无监督数据上进行初步模型训练，使模型能够学习到通用的语言模式、知识表征和统计特征。这一阶段不依赖于特定任务，而是通过在大规模、多样化的数据集上进行广泛的学习，帮助模型建立基础的语言理解能力。通过预训练，模型可以在后续的特定任务上（如分类、生成、翻译等）以更快的速度和更高的准确性进行微调（Fine-tuning）。\n",
    "\n",
    "&emsp;&emsp;预训练的目标是使模型能够有效捕捉数据中的规律，学会高效的特征表示，从而为后续的任务奠定基础。这一阶段通常涉及训练深度神经网络，如Transformer架构，通过处理大规模文本数据，模型能够学会上下文依赖关系、词汇语义关系等复杂的语言特征。\n",
    "\n",
    "&emsp;&emsp;而在选择预训练数据集时，需要特别关注以下几个关键因素：\n",
    "\n",
    "1. **数据规模**  \n",
    "   预训练数据集的规模对模型的性能具有直接影响。大模型通常需要数百亿甚至上千亿个参数，这要求使用海量数据来支持模型训练。在数据量较少的情况下，模型可能无法充分学习复杂的语义关系，进而影响预训练的效果。因此，数据集的规模应足够大，以便涵盖广泛的语言模式和知识。\n",
    "\n",
    "2. **数据多样性**  \n",
    "   数据集的多样性同样至关重要。预训练过程中，模型需要接触各种类型的文本内容，包括新闻、书籍、博客、技术文档等，以确保其能够广泛适应不同语言风格、领域知识和应用场景。如果数据过于单一，模型在后续应用于其他任务时，可能会表现出偏差或局限性。因此，选取多样化的数据源有助于提升模型的泛化能力。\n",
    "\n",
    "3. **数据质量**  \n",
    "   数据质量直接影响模型预训练的有效性和稳定性。高质量的数据应具有较少的噪声、语法错误和不完整的句子。若数据质量较差，模型可能会学习到错误的模式或产生不合理的输出。因此，需对数据集进行预处理和清洗，以剔除错误、冗余或低质量的部分。\n",
    "\n",
    "4. **领域相关性**  \n",
    "   尽管预训练通常是在通用数据集上进行，但在某些情况下，特定领域的数据可能更为重要。例如，若大模型的目标是用于医学或法律领域的应用，预训练数据集中应包含该领域的相关内容，以帮助模型建立更为准确的领域知识。这种数据的领域适配性可以在后续任务中显著提高模型的表现。\n",
    "\n",
    "综上所述，预训练阶段的数据集选取是大模型成功的关键环节之一，良好的数据规模、质量、多样性及领域适配性有助于提高模型的泛化能力和应用效果。同时，数据的合规性与道德责任也不容忽视。通过精心选择和处理数据集，可以为后续的任务微调提供坚实的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c455c-8718-4a47-aa94-a1460f597d2b",
   "metadata": {},
   "source": [
    "#### 当然，你也可以自己构建数据集\n",
    "\n",
    "&emsp;&emsp;在大模型的开发中，构建高质量的预训练数据集是至关重要的一环。如果现有的公开数据集无法完全满足特定需求，研究人员或开发团队可能会选择自构预训练数据集，以便更好地适配模型的任务或领域。在构建过程中，不仅需要考虑数据的收集和处理，还应确保遵循数据的伦理、质量和多样性原则。\n",
    "\n",
    "**构建预训练数据集的步骤**\n",
    "\n",
    "1. **确定数据来源**  \n",
    "   数据集的质量和多样性主要取决于数据来源的选择。常见的数据来源包括：\n",
    "   - **公开数据集**：如维基百科、Common Crawl等大型通用文本数据集。这类数据资源庞大且容易获取。\n",
    "   - **领域特定数据**：从技术文献库、领域文献、研究论文或行业报告中收集数据，适用于特定领域（如医学、法律、金融等）模型的预训练。\n",
    "   - **网络抓取**：通过网络爬虫工具从特定网站抓取数据，尤其适合于需要最新或领域特定信息的场景。需要注意遵守网站的隐私政策及数据使用协议。\n",
    "   - **自有数据**：如企业内部的技术文档、客户服务记录等，这些数据能够使模型专门针对企业应用场景进行优化。\n",
    "\n",
    "2. **数据清洗与预处理**  \n",
    "   收集到的原始数据往往包含很多噪声和冗余信息，如广告、格式错误、拼写错误、重复内容等。为了提高预训练的有效性，必须对数据进行清洗和预处理，主要包括：\n",
    "   - **去重处理**：消除重复的文本片段，以避免模型过度学习某些特定模式。\n",
    "   - **去除噪声**：如无意义的文本、HTML标签、标点符号过度堆积等。这些信息会干扰模型的训练，降低训练效果。\n",
    "   - **文本规范化**：标准化文本格式，如统一的编码格式、消除特殊符号、处理数字或单位等。\n",
    "   - **句子分割与标注**：确保文本中的句子分割合理，特别是对于连续文本段落的处理。根据需要，也可以对文本进行标注（如词性、句法等），以增加模型训练的多样性和丰富性。\n",
    "\n",
    "3. **数据分布与多样性检查**  \n",
    "   数据集的多样性和分布平衡性是影响模型泛化能力的重要因素。应尽量确保不同领域、不同风格、不同语言或不同语境下的数据均有适当的比例，以避免模型在某些特定领域或语言风格上产生偏见。可以通过统计分析工具对数据进行分布检查，确保数据涵盖广泛的上下文和主题。\n",
    "\n",
    "4. **数据格式化与存储**  \n",
    "   预训练数据通常需要转换为模型能够直接使用的格式，如标准化的纯文本文件（.txt）、序列化数据（JSON、CSV等），或者特定的分词和标注格式。确保数据的文件组织结构清晰，便于在训练时进行高效加载。  \n",
    "   此外，预训练数据集往往非常庞大，因此需要合理的存储策略。常见的存储方式包括分片存储、大数据存储系统（如HDFS）、云存储等，以确保在大规模训练时的数据读取速度。\n",
    "\n",
    "5. **数据增强（可选）**  \n",
    "   在某些情况下，数据增强技术可以进一步提高数据集的多样性和丰富性，尤其是在数据规模不足的情况下。常见的数据增强技术包括：\n",
    "   - **同义词替换**：在不改变语义的前提下，用同义词替换句子中的部分词汇。\n",
    "   - **句子顺序打乱**：对文本中的句子顺序进行随机调整，增加训练数据的复杂性。\n",
    "   - **生成式增强**：利用已有模型生成新的语料，通过多种生成方式扩展训练数据的规模。\n",
    "\n",
    "**注意事项**\n",
    "\n",
    "1. **合法性与伦理合规**  \n",
    "   在数据收集的过程中，必须遵守相关法律法规，特别是数据隐私保护和版权问题。未经授权采集的数据可能涉及法律风险，尤其是涉及个人隐私信息或受版权保护的内容时，必须确保有合法使用许可。此外，数据集中应避免包含有害或偏见性的内容，如种族、性别、文化等方面的歧视性语言。\n",
    "\n",
    "2. **数据平衡与去偏**  \n",
    "   数据集中的偏见可能会导致模型输出中的偏差，例如如果训练数据集中某类文本（如某一性别、民族或职业）过度代表，模型可能会倾向于此类文本。为减少这种偏差，数据集的构建应尽量平衡不同类型的内容，确保模型学习到的知识更加中立和广泛。\n",
    "\n",
    "3. **数据质量控制**  \n",
    "   数据的质量直接决定了模型预训练的效果。低质量的数据（如拼写错误、语法错误、不完整的句子等）会导致模型学习到不可靠的模式，因此需要在数据清洗阶段严格控制数据质量。此外，数据标注的准确性也是模型表现的关键，特别是在需要监督信号或标注信息时，必须确保标注的一致性和正确性。\n",
    "\n",
    "4. **数据规模与计算资源匹配**  \n",
    "   自构数据集时需要考虑数据集的规模与计算资源的匹配问题。大规模的数据集需要与之匹配的计算资源，如多GPU/TPU架构、高速存储系统等。若计算资源有限，建议采取渐进式预训练策略，逐步扩大数据集规模或降低模型复杂度，以提高计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cab5b-b7d8-4469-8b73-ffc0c74f06ac",
   "metadata": {},
   "source": [
    "## step 1. 预训练数据集清洗与二进制转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4fed7dc-c2b7-4bd3-99a9-199530c6622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 倒入需要的库\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import psutil\n",
    "import ujson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36508ef8-852f-4ca3-aaf3-cc0dfc578736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的tokenizer词表大小：6400\n"
     ]
    }
   ],
   "source": [
    "## 定义BOS和EOS标记，并加载分词器\n",
    "bos_token = \"<s>\"\n",
    "eos_token = \"</s>\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../models/tokenizer_model/', use_fast = False)\n",
    "print(f\"加载的tokenizer词表大小：{len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96e4c99-438d-4a60-9fc1-0fb02872e3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 行数据：{'text': '祝贺信是为了祝贺生意上的朋友高升或得奖,庆祝收信者在某方面取得的成功而发出的信.可以是写给个人的,也可以是写给团体的.\\n获悉你会经过充分筹备,现已正式成立了.这是我市会计界的一件大喜事.我们谨向你会致以衷心的祝贺!\\nXX会计学会的成立,标志着我市财会战线在社会主义市场经济中起到了推动作用.敬祝你会在今后对提高我市会计科学研究水平,促进与西方会计接轨诸项工作做出更多的贡献.\\n值中技国际招标公司成立10周年之际,谨表示热烈的祝贺.10年来,贵公司在我局利用世界银行及亚洲开发银行贷款项目的招标采购工作中,给予了大力支持与协助.特别在签约及执行合同过程中,坚持信守合同,维护我方用户利益,使项目单位尽快产生效益.借此机会,我们', 'alnum_ratio': 0.9198717949, 'avg_line_length': 78.0, 'char_rep_ratio': 0.0, 'flagged_words_ratio': 0.0, 'industry_type': '金融', 'lang': 'zh', 'lang_score': 0.9711192846, 'max_line_length': 124, 'num_words': 92, 'perplexity': 959.6, 'special_char_ratio': 0.0993589744, 'word_rep_ratio': 0.0, 'id': 5609227288576}\n",
      "第 2 行数据：{'text': '2019年版20元,秀强10元纸币采用光变镂空开窗安全线,与2015年版100元纸币类似,改变钞票观察角度,安全线颜色在红色和绿色之间变化.\\n股份图片来自央行网站2.在现行第五套人民币纸币(2005年版50元,营收亿元20元,营收亿元10元纸币,1999年版1元纸币)防伪技术基础上,50元,20元,10元纸币增加光彩光变面额数字,光变镂空开窗安全线,磁性全埋安全线,竖号码等防伪特征,取消全息磁性开窗安全线和凹印手感线,50元纸币取消光变油墨面额数字,1元纸币增加磁性全埋安全线和白水印.\\n1元硬币规格调整后,秀强直径缩小11%,便于公众携带使用.股份将2005年公告发行的第五套人民币1角硬币称为2005年版第五套人民币1角硬币.公告日后,营收亿元中国人民银行将启动银行在用现金机具升级并适时开展检查工作,无法升级及升级未达标的现金机具将全部停用.\\n2019年版第五套人民币50元,秀强20元,10元纸币票面中部印有光彩光变面额数字,改变钞票观察角度,面额数字颜色出现变化,并可见一条亮光带上下滚动.左侧增加装饰纹样,股份调整横号码,胶印对印图案的样式,取消左下角光变油墨面额数字.\\n钞票纸强度显著提高,营收亿元流通寿命更长.\\n面额数字造型作倾斜处理后,秀强视觉效果更活泼,富有动感,更加突出和醒目.几年后,股份她又跻身政治界,为当时的内阁教育大臣做了6个月的研究员.\\n当时,营收亿元大家就一直在猜王妃为了什么事儿一反常态,气成这样.去年圣诞节前后小报杂志又爆出,秀强 威廉已经变得有点过于接近罗斯,还十分迷恋她.\\n毕竟最近王室实在很忙,股份 这边威廉夫妇的感情危机一触即发,股份 另一边,哈里和梅根也出了幺蛾子! 很可能要移居非洲....... 据\"星期日泰晤士报\"报道,哈里王子和梅根生下第一个小孩后,或许将移居非洲两到三年,俩人将代表英国形象为慈善事业工作.2003年,营收亿元年仅17岁罗斯在意大利度假时,遇到了她现任,比她年长25岁的丈夫.', 'alnum_ratio': 0.8948004837, 'avg_line_length': 103.375, 'char_rep_ratio': 0.0073349633, 'flagged_words_ratio': 0.0, 'industry_type': '金融', 'lang': 'zh', 'lang_score': 0.9324421883, 'max_line_length': 174, 'num_words': 219, 'perplexity': 2000.6, 'special_char_ratio': 0.2031438936, 'word_rep_ratio': 0.0, 'id': 5609227288577}\n",
      "第 3 行数据：{'text': '迎新春的正确姿势:银联\"百福到沪\"海量福利来袭→2023-01-19财富在线佚名|  近日,在上海某商场偶遇了拎着大包小包的李先生,他满脸欣喜地表示:\"年底花钱的地方多,银联\\'百福到沪\\'活动来的很及时,现在又能多省一点.这不,今天来置办了不少年货呢.\"\\n    与此同时,导购员王女士也十分高兴:\"\\'百福到沪\\'的活动力度很大,最近店里顾客明显多了,生意也更好了,今年一定能过个好年!\"\\n    银联\"百福到沪\",打造新春惠民盛会    兔年将至,伴随着年味越来越浓,上海市民们的消费需求也正在蓬勃生长.中国银联携手各大银行,全面开启\"百福到沪\"惠民活动,迎来了一波消费热潮.\\n  除了线下购物满800元减100元,油卡充值满2000元减120元,线上下单,商旅出行,地铁公交等生活中方方面面的消费,中国银联都为魔都市民送上了多重暖心福利.\\n热门商圈逛街购物,享满800元减100元(限1次);满400元减50元(限2次).\\n    在新世界城,大丸百货,梅陇镇伊势丹,汇金百货,太平洋百货,百联世纪购物中心,百联青浦奥特莱斯,巴黎春天等指定活动商圈购物时,通过云闪付APP\"本地专区-更多-魔都好券-今日好券\"领券后,使用银联卡或银联手机闪付完成支付,即可享受优惠.\\n  安行无忧,加油购车实惠多多    活动1.车主加油特惠即日起至2023年1月31日,用户在云闪付APP内领券后,持绑定在云闪付APP中的银联信用卡到指定加油站进行加油卡充值,可享满2000减120元优惠.\\n云闪付APP领券,并前往上海地区指定重点汽车销售4S店消费,享满10000元减200元或满1000元减20元优惠.', 'alnum_ratio': 0.8405172414, 'avg_line_length': 87.0, 'char_rep_ratio': 0.0029112082, 'flagged_words_ratio': 0.0, 'industry_type': '金融', 'lang': 'zh', 'lang_score': 0.8274845481, 'max_line_length': 126, 'num_words': 194, 'perplexity': 2388.6, 'special_char_ratio': 0.2643678161, 'word_rep_ratio': 0.0, 'id': 5609227288578}\n",
      "第 4 行数据：{'text': '买对股票只完成投资过程的一半或更少,接下来就是跟踪个股,并在股价大幅上涨之后卖掉股票,所以如何卖股票同样至关重要.买股票是为了赚钱,但也会让投资者发生亏损.为了避免资金发生大的损失,个人投资者需要学习如何卖股票.这里,我们介绍一个简单实用的卖股票方法. 学习和使用这个办法包括三步: 第一,学习一些有用的卖出规则; 第二,在你所有的市场活动中遵循这些规则; 第三,永远不要违反这些规则. 大牛市!您如何在震荡调整的股市中获取最大利润!通过对股市的研究,现总结了以下五条卖出股票的法则,希望能给大家一些帮助. 卖出法则1:低于买入价7-8%坚决止损 第一个和最重要的一个卖出规则对于许多投资者来讲是很困难的.毕竟对许多人来说,承认自己犯了错误是比较困难的.投资最重要的就在于当你犯错误时迅速认识到错误并将损失控制在最小,这是7%止损规则产生的原因. 通过研究发现40%的大牛股在爆发之后最终往往回到最初的爆发点.同样的研究也发现,在关键点位下跌7-8%的股票未来有较好表现的机会较小.投资者应注意不要只看见少数的大跌后股票大涨的例子.长期来看,持续的将损失控制在最小范围内投资将会获得较好收益. 因此,底线就是股价下跌至买入价的7-8%以下时,卖掉股票!不要担心在犯错误时承担小的损失,当你没犯错误的时候,你将获得更多的补偿.当然,使用止损规则时有一点要注意:买入点应该是关键点位,投资者买入该股时判断买入点为爆发点,虽然事后来看买入点并不一定是爆发点. 卖出法则2:高潮之后卖出股票 有许多方法判断一只牛股将见顶而回落到合理价位,一个最常用的判断方法就是当市场杀上所有投资者都试图拥有该股票的时候.一只股票在逐渐攀升100%甚至更多以后,突然加速上涨,股价在1-2周内上涨25-50%,从图形上看几乎是垂直上升.这种情况是不是很令人振奋?不过持股者在高兴之余应该意识到:该抛出股票了.这只股票已经进入了所谓的高潮区.一般股价很难继续上升了,因为没有人愿意以更高价买入了.突然,对该股的巨大需求变成了巨大的卖压.根据对过去10年中牛股的研究,股价在经过高潮回落之后很难在回到原糕点,如果能回来也需要3-5年的时间. 卖出法则3:连续缩量创出高点为卖出时机 股票价格由供求关系决定.当一只股票股价开始大幅上涨的时候,其成交量往往大幅攀升.原因在于机构投资者争相买入该股以抢在竞争对手的前头.在一个较长时期的上涨后,股价上涨动力衰竭.股价也会会继续创出新高,但成交量开始下降.这个时候就得小心了,这个时候很少有机构投资者愿意再买入该股,供给开始超过需求,最终卖压越来越大.一系列缩量上涨往往预示着反转. 卖出法则4:获利20%以后了结 不是所有的股票会不断上涨的,许多成长型投资者往往在股价上涨20%以后卖出股票.如果你能够在获利20%抛出股票而在7%止损,那么你投资4次对1次就不会遭受亏损.对于这一规则欧内尔给出了一个例外,他指出,如果股价在爆发点之后的1-3周内就上涨了20%,不要卖出,至少持有8周.他认为,这么快速上升的股票有股价上升100-200%的动能,因此需要持有更长的时间以分享更多的收益. 卖出法则5:当一只股票突破最新的平台失败时卖出股票 大家都知道春夏秋冬四季变化,大牛股的走势也有相似的循环.这些股票经历着快速上涨和构筑平台的交替变化.一般来讲,构筑平台的时间越长则股价上升的幅度越大.但这也存在着股价见顶的可能,股价有可能大幅下挫.通常,股价见顶时盈利和销售增长情况非常好,因为股价是反映未来的.无疑,股价将在公司增长迅速放缓之前见顶.当有较大的不利消息时,如果预计该消息将导致最新平台构建失败,投资者应迅速卖出股票. = 更多精彩内容,学习炒股技巧,了解股市行情动态变化,关注掘金视频股市直播间( ejinjie.com)! 每天盘中免费收看实时观点直播,个股解析,盘前分析以及后市策略等信息,助你捕捉最及时的买卖点和操作策略. 文章内容仅供投资者参考,并不构成投资建议.投资者据此操作,风险自担.', 'alnum_ratio': 0.9006622517, 'avg_line_length': 1661.0, 'char_rep_ratio': 0.0, 'flagged_words_ratio': 0.0, 'industry_type': '金融', 'lang': 'zh', 'lang_score': 0.9507360458, 'max_line_length': 1661, 'num_words': 345, 'perplexity': 1628.4, 'special_char_ratio': 0.1505117399, 'word_rep_ratio': 0.0, 'id': 5609227288579}\n",
      "第 5 行数据：{'text': '今日两市震荡翻红,题材股全面回暖.\\n  本周沪指经历四连跌之后,今日震荡走强,题材股也呈全面回暖之势.盘面上,5G,次新股,新零售等板块涨幅居前,银行,券商表现相对低迷.\\n  题材方面,5G概念今日再度受到资金热捧!龙头纵横通信两连板,移为通信,友讯达,万马股份等强势封板.次新股也表现抢眼,药石科技三连板,昨日开板的次新股香飘飘反包涨停,多只前期跌幅较大的次新股涨停,截至收盘,次新股板块超20股涨停.此外,新零售板块也是盘中一大亮点,永辉超市直线封板,带动一众零售股大涨,新华都,中百集团强势涨停.\\n  从本周市场整体情况来看,表现最佳的是国防军工板块,国防军工指数以2.08%的涨幅领跑两市,板块内中兵红箭本周累计上涨13.20%居首位.多数板块本周呈下跌状态,钢铁,采掘,有色金属板块跌幅居前.\\n  本周反弹先锋\\n  从个股表现来看,剔除尚未开板的新股,海虹控股本周累计上涨49.02%居一周涨幅榜首位.该股复牌以来五天收获四个涨停板,周三一字板打开之后,周四成功反包涨停,周五再次连板,提振了市场人气.停牌期间,公司实际控制人变更为国风投基金的实际控制人中国国新控股有限责任公司.\\n  近期开板的次新股药石科技以33.34%的涨幅位居第二名.该股自周三以来走出三连板的行情,打开了次新股的炒作空间.本周药石科技主力资金净流入5.55亿元.\\n  波导股份累计上涨20.33%排名第三位.波导股份系360概念,前期由于360借壳江南嘉捷方案出台,导致股价一路走低,近期超跌反弹,股价七连阳走出一波行情.\\n        主力资金看上了谁\\n  本周主力资金又有哪些新动作?数据显示,本周银行业主力资金净流入居首,主力资金累计净流入35.74亿元.非银金融,食品饮料,建筑装饰主力资金净流入也在10亿元以上.交通运输行业主力资金也呈净流入状态,其余行业主力资金均呈净流出状态.化工,有色金属,计算机,电子行业主力资金净流出均在40亿元以上.\\n  个股方面,中国建筑本周主力资金累计净流入8.16亿元居首,不过该股本周累计下跌1.04%.水泥龙头海螺水泥本周主力资金净流入7.43亿元,本周累计下跌4.20%.此外中国中铁,贵州茅台,海通证券,中国银行,交通银行,伊利股份等权重股本周主力资金净流入也超5亿元.值得注意的是两只次新股中新赛克和药石科技获资金抢筹,本周主力资金净流入也超5亿元.\\n  科大讯飞和士兰微本周主力资金净流出逾10亿元.科大讯飞本周累计下跌近12%,主力净流出逾25亿元居首.士兰微本周主力净流出12亿元,不过该股在弱市行情里走势依然强劲,本周累计上涨7.43%.', 'alnum_ratio': 0.8638025594, 'avg_line_length': 91.1666666667, 'char_rep_ratio': 0.0110599078, 'flagged_words_ratio': 0.0, 'industry_type': '金融', 'lang': 'zh', 'lang_score': 0.7951459289, 'max_line_length': 174, 'num_words': 324, 'perplexity': 2808.5, 'special_char_ratio': 0.21023766, 'word_rep_ratio': 0.0, 'id': 5609227288580}\n"
     ]
    }
   ],
   "source": [
    "## 读取数据部分\n",
    "def preview_dataset(file_path, num_lines=5):\n",
    "    \"\"\"\n",
    "    读取并展示数据集的前 num_lines 行\n",
    "    \"\"\"\n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} 文件不存在，请检查路径！\")\n",
    "\n",
    "    # 逐行读取并展示前 num_lines 行\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for idx, obj in enumerate(reader):\n",
    "            print(f\"第 {idx + 1} 行数据：{obj}\")\n",
    "            if idx + 1 >= num_lines:\n",
    "                break\n",
    "\n",
    "# 指定文件路径和需要展示的行数\n",
    "file_path = \"../data/raw/rank_1241.jsonl\"\n",
    "preview_dataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0946eb2b-bf05-42cf-b396-cd79a31a7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 统计与清理数据\n",
    "def get_total_lines(file_path):\n",
    "    \"\"\"\n",
    "    获取 JSONL 文件的总行数，不忽略错误，保证能够全面统计\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as f:   ## 使用二进制模式避免编码问题\n",
    "        return sum(1 for _ in f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648241c4-ee86-4b1c-bde8-d91421a1cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_jsonl_format(file_path):\n",
    "    \"\"\"\n",
    "    检查 JSONL 文件中的每一行是否是有效的 JSON 格式，带进度显示，并统计所有问题的行\n",
    "    \"\"\"\n",
    "    total_lines = get_total_lines(file_path)\n",
    "    valid_lines = 0\n",
    "    invalid_lines = 0\n",
    "    invalid_loc_line = []\n",
    "\n",
    "    # 使用逐行读取，捕获 JSON 和编码错误\n",
    "    with open(file_path, 'rb') as f:  ## 使用二进制读取避免编码问题\n",
    "        # 使用 tqdm 进度条显示检查进度\n",
    "        for idx, line in tqdm(enumerate(f), total=total_lines, desc=\"Checking JSONL format\"):\n",
    "            try:\n",
    "                # 先尝试将每一行数据解码为 UTF-8\n",
    "                decoded_line = line.decode('utf-8')\n",
    "                # 然后检查是否是有效的 JSON 格式\n",
    "                obj = jsonlines.Reader([decoded_line]).read()\n",
    "                valid_lines += 1\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"Encoding error at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "                invalid_loc_line.append(idx + 1)\n",
    "            except jsonlines.InvalidLineError as e:\n",
    "                print(f\"Invalid JSON at line {idx + 1}: {e}\")\n",
    "                invalid_lines += 1\n",
    "                invalid_loc_line.append(idx + 1)\n",
    "\n",
    "    print(f\"检查完成，文件中共有 {valid_lines} 行有效 JSON 数据，{invalid_lines} 行无效的 JSON 数据。\")\n",
    "    print(f\"无效数据行：{invalid_loc_line}\")\n",
    "    return valid_lines, invalid_lines, invalid_loc_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9588ad7-7019-4831-ab6c-13439a37dff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking JSONL format: 100%|█████████| 594705/594705 [00:15<00:00, 38951.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检查完成，文件中共有 594705 行有效 JSON 数据，0 行无效的 JSON 数据。\n",
      "无效数据行：[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_lines, invalid_lines, invalid_loc_line = check_jsonl_format(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0082e99e-afea-42aa-a134-9d191f47a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_line(file_path, output_path, invalid_line_num):\n",
    "    \"\"\"\n",
    "    读取文件，跳过指定的无效行，并将结果写入新文件\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as infile, open(output_path, 'wb') as outfile:\n",
    "        for idx, line in enumerate(infile):\n",
    "            if idx + 1 != invalid_line_num:  ## 跳过无效行\n",
    "                outfile.write(line)\n",
    "\n",
    "# 因为没有无效行，使用该函数删除第 总行数+1 行，保存为新文件\n",
    "remove_invalid_line(\"../data/raw/rank_1241.jsonl\", \"../data/processed/rank_1241_cleaned.jsonl\", \n",
    "                    invalid_line_num=get_total_lines(\"../data/raw/rank_1241.jsonl\")+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef79961-d267-40f6-b444-d457ca67aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pretrain_data(chunk_size=5000):\n",
    "    \"\"\"\n",
    "    逐块读取 mobvoi_seq_monkey_general_open_corpus.jsonl 文件，\n",
    "    对文本进行分词，并将分词结果保存为二进制文件，支持跳过无效行，并显示处理进度。\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "    chunk_idx = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    # 先计算总行数以便显示进度\n",
    "    with open('../data/processed/rank_1241_cleaned.jsonl', 'r', encoding='utf-8') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "\n",
    "    # 打开jsonlines文件逐行读取\n",
    "    with jsonlines.open('../data/processed/rank_1241_cleaned.jsonl') as reader:\n",
    "        # 使用 tqdm 进度条显示进度\n",
    "        with tqdm(total=total_lines, desc=\"Processing lines\") as pbar:\n",
    "            while True:\n",
    "                try:\n",
    "                    # 使用 itertools.islice 按块读取文件，每次读取 chunk_size 行数据\n",
    "                    chunk = list(itertools.islice(reader, chunk_size))\n",
    "                except jsonlines.InvalidLineError as e:\n",
    "                    print(f\"Skipping invalid chunk at chunk {chunk_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if not chunk:  # 如果读取到文件末尾，则停止\n",
    "                    break\n",
    "\n",
    "                # 遍历块中的每一行数据\n",
    "                # 逐行对数据进行编码（按token进行编码）\n",
    "                for idx, obj in enumerate(chunk):\n",
    "                    try:\n",
    "                        # 从每一行数据中提取'text'字段（即文本内容）\n",
    "                        content = obj.get('text', '')\n",
    "                        \n",
    "                        # 跳过长度超过512的文本\n",
    "                        if len(content) > 512:\n",
    "                            continue\n",
    "\n",
    "                        # 对文本进行分词，将其转为 token ids 序列，并加上BOS和EOS标记\n",
    "                        text_id = tokenizer(f'{bos_token}{content}{eos_token}').data['input_ids']\n",
    "                        \n",
    "                        # 将分词结果添加到 doc_ids 列表中\n",
    "                        doc_ids += text_id\n",
    "\n",
    "                    except UnicodeDecodeError as e:\n",
    "                        # 如果遇到编码错误，跳过该行，并打印错误信息\n",
    "                        print(f\"Skipping invalid line {chunk_idx * chunk_size + idx + 1}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # 每处理完一块数据，更新 chunk_idx 并打印进度信息\n",
    "                chunk_idx += 1\n",
    "                pbar.update(len(chunk))  # 更新进度条\n",
    "\n",
    "                # 如果累积的 token ids 超过 1,000,000 个，保存到文件中\n",
    "                if len(doc_ids) > 1000000:\n",
    "                    arr = np.array(doc_ids, dtype=np.uint16)\n",
    "                    with open(f'../data/processed/clean_rank_1241.bin', 'wb') as f:\n",
    "                        f.write(arr.tobytes())\n",
    "                    doc_ids = []\n",
    "\n",
    "    # 如果处理完所有数据后 doc_ids 中还有未保存的内容，最后再保存一次\n",
    "    if doc_ids:\n",
    "        arr = np.array(doc_ids, dtype=np.uint16)\n",
    "        with open(f'../data/processed/clean_rank_1241.bin', 'wb') as f:\n",
    "            f.write(arr.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1de266-7e3e-4608-9a7d-f93fc9da0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretain_process():\n",
    "    \"\"\"\n",
    "    函数的作用是调用 process_pretrain_data() 函数生成数据，\n",
    "    然后整合所有生成的二进制文件，并将其合并保存为一个总的预训练数据文件。\n",
    "    \"\"\"\n",
    "    # 调用 process_pretrain_data 函数处理数据\n",
    "    process_pretrain_data()\n",
    "\n",
    "    # 数据文件路径列表，目前只处理 rank_1241_cleaned.bin 文件\n",
    "    data_path_list = [\n",
    "        \"../data/processed/clean_rank_1241.bin\"\n",
    "    ]\n",
    "\n",
    "    data_lst = []\n",
    "\n",
    "    # 读取生成的二进制文件\n",
    "    for data_path in data_path_list:\n",
    "        with open(data_path, 'rb') as f:\n",
    "            # 将二进制文件中的内容加载到 numpy 数组中\n",
    "            data = np.fromfile(f, dtype=np.uint16)\n",
    "            data_lst.append(data)\n",
    "\n",
    "    # 将所有读取到的数据合并为一个大数组\n",
    "    arr = np.concatenate(data_lst)\n",
    "    print(f\"合并后的数据大小：{arr.shape}\")\n",
    "\n",
    "    # 后的数据保存为最终的预训练数据文件\n",
    "    with open(\"../data/processed/pretrain_data.bin\", 'wb') as f:\n",
    "        f.write(arr.tobytes())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "358d722e-e294-4dfc-a7fd-170decf95239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|███████████████| 594705/594705 [01:24<00:00, 7054.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的数据大小：(838063,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 运行处理文件\n",
    "pretain_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001663bb-b396-4472-be5b-4c15207ba28e",
   "metadata": {},
   "source": [
    "- **为什么要将数据转换为二进制文件**：\n",
    "\n",
    "1. **高效存储和读取**：二进制文件相比文本文件具有更高的存储和读取效率，尤其是对于大规模的数据集。由于二进制文件是以原始的机器可读格式存储的，不需要进行字符编码转换，因此读取速度更快。对于预训练阶段通常需要处理大量数据，二进制文件可以显著减少读取时间。\n",
    "\n",
    "2. **减少文件大小**：二进制格式的数据比常规的文本格式更紧凑，占用的磁盘空间更少。这对存储大数据集尤其重要，能够显著节省存储资源。\n",
    "\n",
    "3. **与深度学习框架兼容**：深度学习框架（如 PyTorch、TensorFlow 等）在训练时往往需要数据以某种高效的格式加载到内存中。将数据保存为二进制格式有助于快速载入到 NumPy 数组或直接作为模型输入，避免了每次都需要重新转换。\n",
    "\n",
    "4. **跨平台一致性**：二进制文件可以跨平台使用而不丢失精度和数据信息，适合在不同的硬件和操作系统环境中使用。\n",
    "\n",
    "- 这个函数的具体作用：\n",
    "> - `process_pretrain_data()` 函数负责处理原始数据并生成单个或多个二进制文件。\n",
    "> - 然后这些生成的二进制文件通过 `np.fromfile` 读取并存入 NumPy 数组，所有的二进制数据都会被拼接到一起。\n",
    "> - 最后，通过 `np.concatenate` 合并所有的 NumPy 数组，生成一个总的数据数组，并将其存储为一个大的二进制文件 (`pretrain_data.bin`)，供后续的模型预训练使用。\n",
    "\n",
    "总结来说，这种处理方式主要是为了提高效率，方便在大规模预训练任务中快速加载数据并减少磁盘和内存的占用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800e83c-40c5-4abc-b2b4-5a1ed5e88e8c",
   "metadata": {},
   "source": [
    "### 到这里，预训练的数据已经准备好了，接下来就可以开启大模型的预训练过程，此次采用Llama 3架构的文本大模型，之后会尝试GPT架构的，尽情期待"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0e290-acab-45d4-9470-cb8f7158c003",
   "metadata": {},
   "source": [
    "#### Llama 3架构的模型代码放在`/models/`文件夹中的`model_llama.py`文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79560cb4-b0c6-44b9-b855-a5290de272c2",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以下是Llama 3模型架构的具体说明：\n",
    "\n",
    "##### 1. **RMSNorm（均方根标准化层）**\n",
    "&emsp;&emsp;RMSNorm层在该架构中用于对输入进行规范化处理。它通过对每个输入向量的均方根（RMS）进行标准化来减少梯度爆炸或消失的问题。公式为：\n",
    "$[ \\text{RMS}(x) = x / \\sqrt{\\text{mean}(x^2) + \\epsilon} ]$\n",
    "其中`x`是输入，`eps`是防止分母为零的小常数。最后，规范化后的结果会乘以一个可学习的参数`weight`，以实现进一步的尺度调节。\n",
    "\n",
    "##### 2. **预计算位置编码（precompute_pos_cis）**\n",
    "&emsp;&emsp;为了让模型拥有位置感知能力，该函数使用了旋转位置编码（rotary positional embedding），这是一种高效的编码方式。它首先计算一组频率，再通过极坐标生成复数形式的旋转编码。位置编码的核心思想是将序列中的位置信息嵌入到查询（query）和键（key）向量中，从而允许模型捕捉序列的相对位置信息。\n",
    "\n",
    "##### 3. **旋转嵌入（apply_rotary_emb）**\n",
    "&emsp;&emsp;该函数通过将查询和键向量转换为复数，并与预先计算的旋转位置编码进行相乘，来增强模型的相对位置感知能力。最终，它将复数嵌入恢复为实数形式并返回。这种方法减少了位置编码带来的冗余计算，提升了计算效率。\n",
    "\n",
    "##### 4. **多头自注意力机制（Attention）**\n",
    "&emsp;&emsp;多头注意力机制是该模型的核心部分，它允许模型同时关注序列的不同部分。该实现中注意力模块的流程如下：\n",
    "   - **查询、键、值向量的生成**：通过线性变换将输入`x`映射到查询（Q）、键（K）和值（V）空间。\n",
    "   - **旋转嵌入**：将查询和键向量通过旋转位置编码处理。\n",
    "   - **键-值缓存机制（kv_cache）**：在推理过程中，为了加速计算，可以使用缓存的键值对，从而避免重复计算。该机制会在序列长度为1的情况下缓存上次计算的键值对，并将其与新计算的键值对进行拼接，达到加速的效果。\n",
    "   - **注意力权重的计算**：查询和键向量点积后，通过`softmax`得到注意力分数，进而与值向量相乘以获得加权结果。\n",
    "   - **输出处理**：通过线性变换将多头注意力的输出映射回原始维度。\n",
    "\n",
    "&emsp;&emsp;如果环境支持PyTorch 2.0及以上版本，则该注意力模块会优先使用更加高效的闪存注意力（Flash Attention）机制。\n",
    "\n",
    "##### 5. **前馈神经网络（FeedForward）**\n",
    "&emsp;&emsp;前馈神经网络（FFN）是Transformer中的另一重要组件。每个注意力层后都会有一个前馈网络，用于对注意力机制的输出进行非线性变换。在此实现中，FFN首先将输入通过两层线性层并结合`SiLU`激活函数进行非线性处理，然后通过Dropout层引入正则化，防止过拟合。\n",
    "\n",
    "##### 6. **专家选择机制（MoEGate）与专家网络（MOEFeedForward）**\n",
    "&emsp;&emsp;该模型还实现了一个稀疏专家网络（Mixture of Experts, MoE），用于在前馈网络中引入更多的计算能力。稀疏专家机制的关键在于：\n",
    "   - **门控机制**：每个输入会通过门控机制选择最合适的若干个专家来进行处理，专家通过得分函数（如`softmax`）进行选择，模型根据得分选择最合适的`top-k`个专家执行计算。\n",
    "   - **专家执行与权重计算**：被选择的专家根据门控机制的权重对输入进行处理。为了提高计算效率，推理阶段只选择最优的专家，而在训练阶段会重复输入并进行并行计算。\n",
    "   - **辅助损失**：在训练过程中，通过辅助损失（auxiliary loss）来保证专家的均衡使用，避免某些专家被过度利用而另一些专家闲置。\n",
    "\n",
    "这种设计能够提升模型的计算能力和灵活性，尤其适合处理大规模数据和复杂任务。\n",
    "\n",
    "##### 7. **Transformer Block（Transformer块）**\n",
    "&emsp;&emsp;每个Transformer块包含一个注意力层和一个前馈网络，并分别对其输入进行处理。每个块的运行流程如下：\n",
    "   - 输入首先通过`RMSNorm`进行标准化。\n",
    "   - 然后进入注意力层，生成上下文相关的表示。\n",
    "   - 最后，前馈网络处理这些表示，输出结果。\n",
    "\n",
    "如果配置中启用了MoE（稀疏专家），则会使用专家网络进行前馈处理；否则，使用常规的前馈神经网络。\n",
    "\n",
    "##### 8. **Transformer模型的整体架构**\n",
    "&emsp;&emsp;整个Transformer模型由多个Transformer块堆叠而成，模型流程如下：\n",
    "   - **嵌入层**：首先通过嵌入层将输入的token转换为对应的向量表示。\n",
    "   - **位置编码**：通过旋转位置编码为每个输入向量添加位置信息。\n",
    "   - **层堆叠**：输入经过一系列的Transformer块，每一层块都会对输入进行注意力和前馈处理，并逐层传递上下文信息。\n",
    "   - **归一化与输出层**：最后，经过归一化层和线性输出层，将模型的隐藏状态映射为词汇表大小的向量表示，得到最终的预测分布。\n",
    "\n",
    "##### 9. **推理与生成（generate）**\n",
    "&emsp;&emsp;模型的推理流程通过`generate`函数实现，它支持流式生成并可以自适应地选择终止符号（`eos`）结束生成过程。推理时，模型会逐步生成新的token，并根据设定的`温度`、`top-k`策略对输出分布进行控制，从而生成流畅的文本序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0ae62-609c-43bd-a894-f3559a55cc9c",
   "metadata": {},
   "source": [
    "#### 同时，需要编写一个配置文件，也放在`/models/`中，取名叫`LMConfig.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21f3a4-ed71-435c-bb80-33b688481144",
   "metadata": {},
   "source": [
    "其中`LMConfig` 类及其参数详解如下：\n",
    "\n",
    "`LMConfig` 类用于定义和控制模型架构中的一系列关键参数。它继承自`PretrainedConfig`，允许用户通过配置参数来灵活地调整模型的结构和行为。以下是对`LMConfig`中非MOE相关参数的解释，以及这些参数如何影响原始模型架构和运行流程。\n",
    "\n",
    "##### 1. **`dim`（模型的隐藏维度）**\n",
    "   - **默认值**：`512`\n",
    "   - **含义**：这是Transformer模型中每个token的表示向量的维度。`dim`决定了输入和输出的向量大小，也是注意力机制和前馈网络中的主维度。较大的`dim`可以让模型捕捉更多的特征，但也会增加计算和内存开销。\n",
    "   - **影响**：`dim`影响模型中的线性层、注意力机制和前馈神经网络的参数量。在代码中，查询（Q）、键（K）、值（V）等向量的维度均与此参数相关联。如果`dim`增加，则这些向量的维度和计算量也会增加。\n",
    "\n",
    "##### 2. **`n_layers`（Transformer层数）**\n",
    "   - **默认值**：`8`\n",
    "   - **含义**：这是模型中堆叠的Transformer层的数量。每一层由一个多头注意力机制和一个前馈神经网络组成。更多的层数意味着模型能够处理更复杂的特征和模式，但同时也会增加模型的训练时间和推理时间。\n",
    "   - **影响**：`n_layers`直接决定了模型的深度。在`Transformer`类的构造函数中，通过一个循环来堆叠多个`TransformerBlock`，层数越多，模型就会变得越深，能够处理的语义信息更加丰富。\n",
    "\n",
    "##### 3. **`n_heads`（注意力头的数量）**\n",
    "   - **默认值**：`16`\n",
    "   - **含义**：这是多头注意力机制中头的数量。注意力头是指将输入向量拆分成多个子空间，并在每个子空间中分别进行注意力计算，最后将这些子空间的结果拼接起来。更多的头可以让模型关注到输入序列的不同部分和不同特征。\n",
    "   - **影响**：`n_heads`影响每个注意力头中分配的维度大小。在代码中，通过线性变换将输入映射为多个头，每个头的维度为`dim / n_heads`。增加头的数量可以让模型在更多的子空间中平行地处理信息，但计算复杂度也会增加。\n",
    "\n",
    "##### 4. **`n_kv_heads`（键值头的数量）**\n",
    "   - **默认值**：`8`\n",
    "   - **含义**：在多头注意力机制中，键（Key）和值（Value）向量的头数。`n_kv_heads`表示键值向量会分配到的头数。默认情况下，键值头的数量等于查询头的数量（即`n_heads`），但可以通过该参数将其分开设置。\n",
    "   - **影响**：该参数决定了键和值在注意力机制中的分配方式。如果`n_kv_heads`与`n_heads`不同，模型会按照`n_kv_heads`的数量来处理键值向量，可能会影响模型处理长序列时的记忆能力。\n",
    "\n",
    "##### 5. **`vocab_size`（词汇表大小）**\n",
    "   - **默认值**：`6400`\n",
    "   - **含义**：这是模型可以处理的词汇表的大小。词汇表是模型可以输入的词或token的集合。`vocab_size`决定了输入嵌入层和输出层的大小，它对应于模型的输入token和输出词预测的范围。\n",
    "   - **影响**：`vocab_size`影响嵌入层的大小。在`Transformer`模型中，`tok_embeddings`和`output`层的维度是由`vocab_size`决定的。如果词汇表大小增加，模型处理的token种类也会增加，但也会增加计算和存储的需求。\n",
    "\n",
    "##### 6. **`hidden_dim`（前馈网络隐藏层的维度）**\n",
    "   - **默认值**：`None`\n",
    "   - **含义**：这是前馈神经网络（FeedForward Network, FFN）中隐藏层的维度。默认情况下，该值为`4 * dim`，即每个Transformer块中的前馈网络的隐藏层维度是模型维度的4倍。该参数允许用户手动指定前馈网络的隐藏层维度。\n",
    "   - **影响**：`hidden_dim`影响FFN的计算复杂度和模型的表示能力。如果该值过小，模型可能无法捕捉足够复杂的特征；如果过大，计算量和参数量将显著增加。\n",
    "\n",
    "##### 7. **`multiple_of`（前馈网络维度的倍数）**\n",
    "   - **默认值**：`64`\n",
    "   - **含义**：该参数确保前馈网络中隐藏层的维度是某个整数的倍数。这是为了提高计算效率，尤其是在并行计算中，这种对齐操作能够优化内存访问和计算性能。\n",
    "   - **影响**：`multiple_of`与`hidden_dim`一起影响前馈网络的维度。在代码中，通过将`hidden_dim`调整为`multiple_of`的倍数，确保其符合硬件加速的要求。\n",
    "\n",
    "##### 8. **`norm_eps`（归一化层中的epsilon）**\n",
    "   - **默认值**：`1e-5`\n",
    "   - **含义**：这是用于归一化层中的一个小常数，以防止分母为零。通常用于`RMSNorm`和其他标准化操作中，确保在数值计算时不会出现除以零的情况。\n",
    "   - **影响**：`norm_eps`直接影响`RMSNorm`操作的稳定性。较小的`eps`可以提高精度，但也容易引发数值不稳定。合理设置`eps`值可以在数值稳定性和计算精度之间取得平衡。\n",
    "\n",
    "##### 9. **`max_seq_len`（最大序列长度）**\n",
    "   - **默认值**：`512`\n",
    "   - **含义**：这是模型能够处理的最大输入序列长度。`max_seq_len`决定了位置编码和注意力矩阵的最大尺寸。超过这个长度的输入将被截断，或者需要通过分块处理长序列。\n",
    "   - **影响**：`max_seq_len`影响模型处理长序列时的能力和效率。如果该值设置较大，模型能够处理更长的上下文信息，但会增加注意力矩阵的计算开销和内存需求。\n",
    "\n",
    "##### 10. **`dropout`**\n",
    "   - **默认值**：`0.0`\n",
    "   - **含义**：这是在训练过程中防止过拟合的一种正则化技术。在模型的各个层中，会以一定的概率随机将一些神经元的输出置为零，以防止模型对训练数据过度拟合。`dropout`的值表示丢弃神经元的概率。\n",
    "   - **影响**：`dropout`值越高，模型的正则化效果越强，但过高的`dropout`值可能会导致模型难以学习复杂的模式。如果设置为`0.0`，则表示不使用Dropout层。\n",
    "\n",
    "##### 11. **`flash_attn`**\n",
    "   - **默认值**：`True`\n",
    "   - **含义**：这是一个布尔参数，用于指定是否使用闪存注意力机制（Flash Attention）。闪存注意力是一种更高效的注意力计算方法，特别适用于长序列和大模型的场景中，可以显著降低内存使用并加速计算。\n",
    "   - **影响**：如果设置为`True`，模型将尝试使用更快的注意力实现，前提是当前环境支持（例如需要PyTorch >= 2.0）。这将提高计算效率，特别是在处理长序列时。如果设置为`False`，则使用标准的注意力计算方式，速度较慢但兼容性更高。\n",
    "\n",
    "##### 参数的整体影响\n",
    "&emsp;&emsp;这些非MOE相关的参数决定了模型的基础架构，影响了模型的深度、宽度、注意力机制、前馈网络的复杂性以及模型的正则化方式。通过调整这些参数，用户可以定制模型的计算资源需求、精度和效率。例如，增加`dim`和`n_layers`可以提高模型的表达能力，但也会增加计算复杂度和内存需求；而`dropout`和`norm_eps`等参数则帮助模型在训练中更稳定地学习并避免过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386ee28-9eb9-4f21-9644-2ac82a4a9db5",
   "metadata": {},
   "source": [
    "#### 数据读取和脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a1734-b0a4-4ca2-bcdb-94e801e97608",
   "metadata": {},
   "source": [
    "我们在`/models/`文件夹内创建一个名为`dataset.py`的脚本，用于在训练时读取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981a311-6c15-4ca0-aa48-361bfccfca20",
   "metadata": {},
   "source": [
    "## 到了这一步就正式可以预训练了，首先编写预训练脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa8de9-aca8-494f-8a44-7fa84c76fb64",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在准备好了模型架构和参数代码之后，接下来我们就可以开始编写模型的预训练脚本了，该脚本的核心功能是将数据带入模型，并且根据模型参数来完成模型的预训练过程。这里还是一样，我们首先需要在`项目顶层目录`下创建一个预训练脚本文件`pretrain.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4a88e-f5e4-4a22-a677-649c14cd021f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;`Pretain.py`这段脚本用于启动语言模型的预训练过程，结合了`PyTorch`的多GPU并行训练（`DistributedDataParallel`）、梯度累积、学习率调度等功能，以优化大规模的训练任务。脚本的主要功能包括模型初始化、数据加载、分布式训练支持以及训练日志记录。下面详细解释代码中的核心功能和参数设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc48fb-3757-4e31-8b6b-a134679ee0f2",
   "metadata": {},
   "source": [
    "##### 脚本功能解释如下\n",
    "\n",
    "1. **分布式训练支持 (`DistributedDataParallel`)**\n",
    "   - 脚本可以通过分布式训练模式来加速模型训练。在分布式模式下，模型会通过多GPU并行计算实现数据并行。`init_distributed_mode`函数负责初始化分布式环境，使用`NCCL`后端进行通信。\n",
    "   - 使用`torch.distributed`模块的`DistributedSampler`确保数据在不同GPU之间合理分配，避免数据重叠。\n",
    "\n",
    "2. **模型初始化 (`init_model`)**\n",
    "   - 该函数初始化模型，并打印模型的参数数量。在配置文件`LMConfig`中设置的参数（如`dim`、`n_layers`等）将被用于初始化`Transformer`模型。参数的设置将影响模型的规模和计算量。\n",
    "   - 如果启用了`MOE`（专家模型），则会根据配置初始化`MOE`相关的组件。\n",
    "   - 该模型支持在PyTorch 2.0及以上版本中通过`torch.compile`进一步优化模型的推理性能。\n",
    "\n",
    "3. **学习率调度 (`get_lr`)**\n",
    "   - 学习率调度器采用了余弦退火（Cosine Annealing）的方式来逐步降低学习率。在训练的早期阶段进行学习率预热（warm-up），然后逐步减小学习率，最终在训练结束时接近于零。这个策略可以在训练初期加快收敛速度，并在后期稳定地进行微调。\n",
    "   - 学习率由`get_lr`函数动态计算，在每一步训练中通过迭代更新学习率。\n",
    "\n",
    "4. **训练过程 (`train_epoch`)**\n",
    "   - 每个`epoch`的训练过程包括以下步骤：\n",
    "     - 将训练数据加载到设备上。\n",
    "     - 动态调整学习率。\n",
    "     - 使用`torch.cuda.amp.autocast()`自动进行混合精度训练，减少显存使用，提高计算效率。\n",
    "     - 梯度累积：模型在每`accumulation_steps`步之后更新参数，以减少显存开销，适用于大模型和小批次训练。\n",
    "     - 梯度裁剪：为了防止梯度爆炸，通过`torch.nn.utils.clip_grad_norm_`对梯度进行裁剪。\n",
    "     - 训练日志记录：定期记录训练过程中的损失、学习率和时间，并将结果输出到控制台。\n",
    "\n",
    "5. **模型保存**\n",
    "   - 在每`save_interval`步后保存模型的状态字典。保存时，根据配置选择是否包括`MOE`专家模型的参数。\n",
    "   - 如果是在分布式训练模式下（`DistributedDataParallel`），则会保存主进程模型的参数。\n",
    "\n",
    "6. **梯度放大与更新 (`GradScaler`)**\n",
    "   - 通过`torch.cuda.amp.GradScaler`来缩放梯度，进一步减少数值不稳定的风险并提高混合精度训练的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14090899-87c6-4130-9ca9-7b296ec85693",
   "metadata": {},
   "source": [
    "#### 脚本参数解释如下：\n",
    "\n",
    "1. **`--out_dir`**\n",
    "   - **默认值**：`\"out\"`\n",
    "   - **含义**：模型保存的输出目录。预训练过程中生成的模型权重会保存在该目录下。\n",
    "\n",
    "2. **`--epochs`**\n",
    "   - **默认值**：`20`\n",
    "   - **含义**：训练的总轮数。每个轮次将使用整个训练数据集进行一次完整的模型更新。\n",
    "\n",
    "3. **`--batch_size`**\n",
    "   - **默认值**：`32`\n",
    "   - **含义**：训练时使用的批次大小。较大的批次可以提高模型的泛化性能，但也需要更多的计算资源。\n",
    "\n",
    "4. **`--learning_rate`**\n",
    "   - **默认值**：`2e-4`\n",
    "   - **含义**：初始学习率，控制模型的权重更新速度。余弦退火调度器将会逐步减少该值。\n",
    "\n",
    "5. **`--device`**\n",
    "   - **默认值**：`\"cuda:0\"`（如果有GPU），否则为`\"cpu\"`\n",
    "   - **含义**：训练设备。可以是`CPU`或`GPU`，通常在大规模训练中使用GPU。\n",
    "\n",
    "6. **`--dtype`**\n",
    "   - **默认值**：`\"bfloat16\"`\n",
    "   - **含义**：指定数据类型（如`float16`或`bfloat16`）以启用混合精度训练。`bfloat16`通常在GPU上使用，因为它可以提高训练速度并减少显存占用。\n",
    "\n",
    "7. **`--use_wandb`**\n",
    "   - **默认值**：`False`\n",
    "   - **含义**：是否使用`Weights & Biases`（WandB）工具来跟踪实验进度和记录训练指标。WandB 是一种实验跟踪和可视化工具。\n",
    "\n",
    "8. **`--wandb_project`**\n",
    "   - **默认值**：`\"MateConv-Pretrain\"`\n",
    "   - **含义**：如果使用WandB，这是用于实验的项目名称。\n",
    "\n",
    "9. **`--num_workers`**\n",
    "   - **默认值**：`8`\n",
    "   - **含义**：用于数据加载的工作进程数量。更多的`num_workers`可以加速数据加载，但会占用更多的CPU资源。\n",
    "\n",
    "10. **`--data_path`**\n",
    "    - **默认值**：`\"./dataset/pretrain_data.bin\"`\n",
    "    - **含义**：训练数据集的路径。该数据集应该是预处理后的二进制文件，用于模型预训练。\n",
    "\n",
    "11. **`--ddp`**\n",
    "    - **默认值**：`False`\n",
    "    - **含义**：是否启用分布式数据并行（DistributedDataParallel）训练。如果启用，模型将分布到多个GPU上进行并行计算。\n",
    "\n",
    "12. **`--accumulation_steps`**\n",
    "    - **默认值**：`8`\n",
    "    - **含义**：梯度累积步数。通过累积多个批次的梯度，再进行一次参数更新，以减少显存压力并提高训练稳定性。\n",
    "\n",
    "13. **`--grad_clip`**\n",
    "    - **默认值**：`1.0`\n",
    "    - **含义**：梯度裁剪的阈值，用于限制梯度的最大范数，防止梯度爆炸。\n",
    "\n",
    "14. **`--warmup_iters`**\n",
    "    - **默认值**：`0`\n",
    "    - **含义**：学习率预热的迭代次数。在预热阶段，学习率从零逐渐增加到设定值。\n",
    "\n",
    "15. **`--log_interval`**\n",
    "    - **默认值**：`100`\n",
    "    - **含义**：日志记录的间隔步数。每经过`log_interval`步会在控制台输出一次损失和学习率等信息。\n",
    "\n",
    "16. **`--save_interval`**\n",
    "    - **默认值**：`1000`\n",
    "    - **含义**：模型保存的间隔步数。每经过`save_interval`步，模型的当前状态将被保存到指定的输出目录中。\n",
    "\n",
    "17. **`--local_rank`**\n",
    "    - **默认值**：`-1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1780766-2d72-4f09-98e2-81eb6bfb96bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
